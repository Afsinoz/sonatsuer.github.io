<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>The blog of Sonat Süer</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000"/>
 <updated>2018-07-23T22:11:20+03:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Sonat Süer</name>
   <email></email>
 </author>

 
 <entry>
   <title>An Invitation to Functional Programming (For Mathematicians)</title>
   <link href="http://localhost:4000/evangelism/2018/07/23/invitation.html"/>
   <updated>2018-07-23T21:39:30+03:00</updated>
   <id>http://localhost:4000/evangelism/2018/07/23/invitation</id>
   <content type="html">&lt;p&gt;(This post is based on two talks I gave at the mathematics departments of Bilkent University and METU.)&lt;/p&gt;

&lt;h1 id=&quot;functional-programming&quot;&gt;Functional Programming&lt;/h1&gt;

&lt;p&gt;Here is the definition of functional programing in Wikipedia:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In computer science, functional programming is a programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This should be very appealing to mathematicians. The reason is that choosing pure –i.e. mathematical– functions as a foundation means that programs are &lt;em&gt;algebraic&lt;/em&gt; objects. Computation is literally simplification of algebraic expressions.&lt;/p&gt;

&lt;p&gt;This paradigm is very intuitive from the point of view of a mathematician. For instance, we can use &lt;em&gt;substituition of equals for equals principle&lt;/em&gt;, a principle we learn in primary school, as a way to reason bout programs. This is also very useful from the point of view of a programmer. It even has a fancy name in computer science. It is called &lt;em&gt;referential transparency&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;However, somewhat surprisingly, functional programming is considered difficult by a lot of programmers. Personally, I think programmers find it difficult because first they need to &lt;em&gt;unlearn&lt;/em&gt; an entire paradigm which they have been using for years. This is where the mathematicians have the advantage. A mathematician only needs to &lt;em&gt;learn&lt;/em&gt; the specifics of a programming language based on ideas with which he/she is already familiar. Learning is gnerally easier than unlearning.&lt;/p&gt;

&lt;p&gt;In this post, I will try to lure you, my dear mathematician, into functional programming.&lt;/p&gt;

&lt;h1 id=&quot;enter-haskell&quot;&gt;Enter Haskell&lt;/h1&gt;

&lt;p&gt;We will work out an example of the &lt;em&gt;programs are algebraic objects&lt;/em&gt; approach in detail. For that we will use Haskell, an industrial strength pure functional language, named after the mathematician and logician Haskell Curry. This will not be a crash course in Haskell. Instead, I will explain the syntax as we go.&lt;/p&gt;

&lt;p&gt;Like the vast majority of programming languages, Haskell has data structures. One of the most common data structures in Haskell is the  list structure. Here is the definition: given a type &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;, (like integers, floating point numbers, strings, etc.) we define &lt;code class=&quot;highlighter-rouge&quot;&gt;[a]&lt;/code&gt; by &lt;em&gt;structural recursion&lt;/em&gt; as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[]&lt;/code&gt; is a list, it is called the empty list;&lt;/li&gt;
  &lt;li&gt;if &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is an element of type &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; is a list of elements of type &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; then &lt;code class=&quot;highlighter-rouge&quot;&gt;x : xs&lt;/code&gt; is also list.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1 : (2 : (3 : [])) = 1 : 2 : 3 : []
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;is a list of integers. Haskell allows us to express this list as &lt;code class=&quot;highlighter-rouge&quot;&gt;[1, 2, 3]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note that here &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; is a actually &lt;em&gt;binary operation&lt;/em&gt; and a list actually a &lt;em&gt;term&lt;/em&gt;. Now let us define a simple function on lists.&lt;/p&gt;

&lt;p&gt;Let &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ys&lt;/code&gt; be two lists (over the same type). We define the concatenation &lt;code class=&quot;highlighter-rouge&quot;&gt;xs++ys&lt;/code&gt; of &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ys&lt;/code&gt; by recursion on the structure of &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[]++ys = ys    and    (x : xs') ++ ys = x : (xs' ++ ys).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let us compute an example:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[1,2]++[3,4] = (1 : (2 : [] )) ++ (3 : (4 : []))
             = 1 : (( 2 : []) ++ (3 : (4 : []))
             = 1 : (2 : ([] ++ (3 : (4 : [])))
             = 1 : (2 : (3 : (4 : [])))
             = [1,2,3,4]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note the simialarity between this calculation and the calculations you do in algebra.&lt;/p&gt;

&lt;h1 id=&quot;our-first-program-in-haskell&quot;&gt;Our First Program in Haskell&lt;/h1&gt;

&lt;p&gt;Suppose that we want to devise a function –that is to say, write a program– which produces the reverse of a list. Here is the obvious solution. Given a list &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; let us define &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;naiveReverse [] = []
naiveReverse (x : xs) = naiveReverse xs ++ [x]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This looks like a (recursive) mathematical definition but it is actually valid Haskell code. As an exercise you may want to show that &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse [1,2,3] = [3,2,1]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our program solves the problem of inveting a list, however we call it the naive reverse for a reason. It is not very efficient. First, note that the computation of &lt;code class=&quot;highlighter-rouge&quot;&gt;xs ++ ys&lt;/code&gt; requires length of &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; many applications of &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; operation. This causes the computtaion to &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse xs&lt;/code&gt; to require \(\frac{1}{2}n(n-1)\) applications of the operation &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; where \(n\) is the length of &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can easily prove this by induction on \(n\). Base case is trivial. Suppose the statement holds for \(n\). Let &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; be of length \(n\). Then, for any &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; we have
\[
{\rm naiveReverse}\, (x : xs) = {\rm naiveReverse}\, xs ++ [x].
\]
As &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse xs&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;xs&lt;/code&gt; has the same length, namely \(n\), computation of &lt;code class=&quot;highlighter-rouge&quot;&gt;++&lt;/code&gt; takes \(n\) applications of the &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; operation. On the other hand, by induction hypothesis, &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse xs&lt;/code&gt; requires \(\frac{1}{2}n(n-1)\) applications. Adding these two yields \(\frac{1}{2}n(n+1)\), which is what we want.&lt;/p&gt;

&lt;p&gt;So the complexity of &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse&lt;/code&gt; is quadratic as a function of the input length because &lt;code class=&quot;highlighter-rouge&quot;&gt;++&lt;/code&gt; is expensive. There is a very similar situation in mathematics with an elegant solution: multiplication is expensive, however one can use logarithms to turn multiplication into addition, do the additon and come back with exponentiaiton. This is why logarithm tables were used before computers.&lt;/p&gt;

&lt;p&gt;We will use the same trick to optimize &lt;code class=&quot;highlighter-rouge&quot;&gt;naiveReverse&lt;/code&gt;, that is, we will change the underlying monoid using a homomorphism.&lt;/p&gt;

&lt;h1 id=&quot;algebra-refresher&quot;&gt;Algebra Refresher&lt;/h1&gt;
&lt;p&gt;Recall that a monoid is a triple \((M, \cdot, 1_M)\) where \(\cdot\) is a binary associative operation on \(M\) and \(1_M\) is the identity element of \(\cdot\).&lt;/p&gt;

&lt;p&gt;Here are a few examples:
\begin{enumerate}
	\onslide&amp;lt;2-&amp;gt;{\item For any set $S$, the set of self maps of $S$, denoted by ${\rm End}(S)$ is a monoid under composition. The identity is the identity function.}
	\onslide&amp;lt;3-&amp;gt;{\item For any set $S$, the set of finite sequences with elements from $S$ form a monoid under concatenation. The identity is the empty list. (Actually this is called the free monoid generated by $S$.)}
\end{enumerate}
\end{frame}&lt;/p&gt;

&lt;p&gt;A monoid homomorphism from \((M, \cdot, 1_M)\) to \((N, \circ, 1_N)\) is a function \(\varphi : M \to N\) such that
\begin{frame}
\frametitle{Algebra refresher: Cayley representation theorem}
\begin{theorem}{\bfseries Cayley} A monoid with underlying set $S$ can be embedded in ${\rm End}(S)$.
\pause
\begin{proof}
One can easily check that $\mathcal{C}(s)=\lambda_s$ where $\lambda_s(x)= s * x$ is such an embedding.
\end{proof}
\end{theorem}
\vspace*{1em}
\pause&lt;/p&gt;

&lt;p&gt;Note that if a function $f$ is in the image of $\mathcal{C}$ then one can recover the element it came from by applying $f$ to the identity of the monoid.&lt;/p&gt;

&lt;p&gt;\end{frame}&lt;/p&gt;

&lt;p&gt;\begin{frame}
\frametitle{Pushing the problem to ${\rm End}([a])$}
In ${\rm End}([a])$, the monoidal operation, namely function composition, is very cheap. To be more precise, it requires constant time because the composition of two functions is left untouched untill someone tries to apply it to a value.
\vspace*{1em}
\pause&lt;/p&gt;

&lt;p&gt;However, the notion of reverting only makes sense in $[a]$ and not in ${\rm End}([a])$. So we need to embed only the concatenation part of the problem into ${\rm End}([a])$.
\vspace*{1em}
\pause&lt;/p&gt;

&lt;p&gt;The inverse of $xs$ is given by $f\, xs\, []$ where $f$ is defined by
[
f\, [] = id
\;\;\;\text{ and }\;\;\;\;
f\, (x : xs) = f\, xs \circ \mathcal{C}([x]).
]
\end{frame}&lt;/p&gt;

&lt;p&gt;\begin{frame}[fragile]
\frametitle{Haskell Implementation}
\begin{verbatim}
type End s = s -&amp;gt; s&lt;/p&gt;

&lt;p&gt;singleton :: a -&amp;gt; End [a]
singleton x = f where f y = x : y&lt;/p&gt;

&lt;p&gt;cayleyReverse :: [a] -&amp;gt; End [a]
cayleyReverse [] = id
cayleyReverse (x : xs) = cayleyReverse xs . singleton x&lt;/p&gt;

&lt;p&gt;naiveReverse :: [a] -&amp;gt; [a]
naiveReverse [] = []
naiveReverse (x : xs) = naiveReverse xs ++ [x]&lt;/p&gt;

&lt;p&gt;betterReverse :: [a] -&amp;gt; [a]
betterReverse xs = cayleyReverse xs []
\end{verbatim}
\end{frame}&lt;/p&gt;

&lt;p&gt;\section{Sketch of a Vast Generalization}
\begin{frame}
\frametitle{Monoid Objects in a Category}
We will port our method to different domains using category theory.&lt;/p&gt;

&lt;p&gt;\pause
First, we need to translate our notions to category theory. Effectively, this means expressing \emph{everything} in terms of functions and their compositions.&lt;/p&gt;

&lt;p&gt;We need
\begin{itemize}
	\onslide&amp;lt;3-&amp;gt;{\item Elements (using terminal objects)}
	\onslide&amp;lt;4-&amp;gt;{\item Products (expressing Cartesian product as a categorical limit)}
	\onslide&amp;lt;5-&amp;gt;{\item Monoids (using the ``associativity’’ of Cartesian products)}
\end{itemize}&lt;/p&gt;

&lt;p&gt;\onslide&amp;lt;6-&amp;gt;{
Now a monoid is a set $M$ together with two functions $*\colon M\times M\to M$ and $e\colon 1\to M$ satisfying the associativity and identitiy rules.
}
\vspace{1em}&lt;/p&gt;

&lt;p&gt;\onslide&amp;lt;7-&amp;gt;{
The associativity rule of a monoid $M$ can be expressed by saying that the two functions
[
&lt;em&gt;\, \circ \, (id \times\,&lt;/em&gt;)
\;\;\;\text{ and }\;\;\;
&lt;em&gt;\,\circ \, (&lt;/em&gt;\times id) \circ \alpha
]
from $M\times(M\times M)$ to $M$ are equal. (Here $\alpha (x,(y, z)) = ((x,y),z))$.)
}&lt;/p&gt;

&lt;p&gt;\onslide&amp;lt;8-&amp;gt;{
\vspace{1em}
Exercise: Express the identity rule.}
\end{frame}&lt;/p&gt;

&lt;p&gt;\begin{frame}
\frametitle{Monoid Objects in a Monoidal Category}
This is where the hand-waving starts. For actual definition you may use Wikipedia.
\vspace{1em}&lt;/p&gt;

&lt;p&gt;\pause
First, let us forget about sets and functions and work with an arbitrary category. This means that we work with abstract objects and abstract ``maps’’ between them. The composition $f \circ g$ is defined only when the codomain of $g$ coincides with the domain of $f$. The only assumption about this abstract composition is associativty.
\vspace{1em}&lt;/p&gt;

&lt;p&gt;\pause
Let us also forget about Cartesians product and work with an arbitrary monoidal structure. This means that we have a binary construction on our objects with a function like $\alpha$ –and a little bit more.
\vspace{1em}&lt;/p&gt;

&lt;p&gt;\pause
It turns out that the notion of a monoid can be expressed in any category with a monoidal structure. If your category is the category of sets and functions between them and the monoidal structure is the Cartesian product then you recover the original notion.
\vspace{1em}&lt;/p&gt;

&lt;p&gt;\end{frame}&lt;/p&gt;

&lt;p&gt;\begin{frame}
\frametitle{Examples: Applicatives, Monads and Arrows}
But is this generalization useful?
\pause You bet! Here are some examples.
\begin{itemize}
	\onslide&amp;lt;2-&amp;gt;{\item Let $Hask$ be the category of types in Haskell with functions definable in Haskell and pairing (or product) as the monoidal structure. (From a strict point of view, this is not true but good enough to snatch ideas from category theory.) Then for any $a$, $[a]$ and $End\, a$ are monoids.}
	\onslide&amp;lt;3-&amp;gt;{\item Consider endofunctors of $Hask$ with composition as the monoidal structure. Then the monoids are called the monads. The Cayley representation in this context is acalled the codensity monad transformation for historical reasons.}
	\onslide&amp;lt;4-&amp;gt;{\item In the category of endofunctors of $Hask$ with Day convolution as the monoidal structure the monoids are called the applicatives.}
	\onslide&amp;lt;5-&amp;gt;{\item In the category of profunctors with profunctor tensor as the monoidal structure the monoids are called the weak arrows.
	\item \ldots}
\end{itemize}
\end{frame}&lt;/p&gt;

&lt;p&gt;\begin{frame}
\frametitle{Tip of the iceberg}
This was just one idea and its generalizations. Here is an incomplete list of other relevant pure mathematical notions, each of which could be the topic of a different talk:
\begin{itemize}
	\item Free constructions (free monads, free applicatives, etc.)
	\onslide&amp;lt;2-&amp;gt;{\item Categorical duality (comonds, cofree constructions etc.)}
	\onslide&amp;lt;3-&amp;gt;{\item Functors (adjointness and representability)}
	\onslide&amp;lt;4-&amp;gt;{\item Categories as databases (AQL and functorial migration)}
	\onslide&amp;lt;5-&amp;gt;{\item Curry-Howard correspondence
	\item \ldots}
\end{itemize}
\onslide&amp;lt;5-&amp;gt;{A lot of exciting things are happenning here!}
\end{frame}&lt;/p&gt;

&lt;h1 id=&quot;final-remarks&quot;&gt;Final Remarks&lt;/h1&gt;
&lt;p&gt;There is nothing more practical than a good theory.&lt;/p&gt;

&lt;p&gt;In theory, there is no difference between theory and practise. In practise, there is.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Self Aware Programs</title>
   <link href="http://localhost:4000/recursion-theorem/2018/07/11/self-aware-programs.html"/>
   <updated>2018-07-11T00:00:00+03:00</updated>
   <id>http://localhost:4000/recursion-theorem/2018/07/11/self-aware-programs</id>
   <content type="html">&lt;h1 id=&quot;a-classical-exercise&quot;&gt;A Classical Exercise&lt;/h1&gt;

&lt;p&gt;A computer program which produces its own source code is called a quine, named after the logician Willard Van Orman Quine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Write a quine in your favourite programming language.&lt;/p&gt;

&lt;p&gt;If you haven’t solved this exercise before, I urge you to stop right now and give it a try.&lt;/p&gt;

&lt;p&gt;To give you an idea of why this is an interesting problem, let us try to solve it in a naive way. Instead of using an actual programming language
I will use pseudocode to keep things simple.&lt;/p&gt;

&lt;p&gt;The obvious candidate of a quine is&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Print your own source code
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This looks like cheating but actually there are languages which allow this. For instance&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;10 List
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;is a valid GWBasic program which, when run, prints &lt;code class=&quot;highlighter-rouge&quot;&gt;10 List&lt;/code&gt; on the screen. Actually, in GWBasic, &lt;em&gt;any&lt;/em&gt; code which starts with &lt;code class=&quot;highlighter-rouge&quot;&gt;10 List&lt;/code&gt;
is a quine.&lt;/p&gt;

&lt;p&gt;So, to make things less trivial and more generic, let us restrict ourselves to text manipulations. In this case, the first candidate is&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Print &quot;Print&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output of this code is simply &lt;code class=&quot;highlighter-rouge&quot;&gt;Print&lt;/code&gt;. So it doesn’t work. Seeing this, you may try&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Print &quot;Print &quot;Print&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This time the output is &lt;code class=&quot;highlighter-rouge&quot;&gt;Print &quot;Print&quot;&lt;/code&gt;. Failed again. Actaully, any code of the form &lt;code class=&quot;highlighter-rouge&quot;&gt;Print &quot;&amp;lt;any kind of fixed text&amp;gt;&quot;&lt;/code&gt; will fail
as the source code is strictly longer than the text it prints. Now here is an idea to circumvent this: We can use the the given text, or its parts, more than once!
So let us try a program like this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;&amp;lt;there is going to be a text here&amp;gt;&quot;
&amp;lt;there are going to be commands here explaining what to do with A&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we will construct the source code using the parts of the text A, the first line of the code should be a part of A. So our program should look like this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;Let A be the following text:
&amp;lt;there are going to be more lines here&amp;gt;&quot;
&amp;lt;there are going to be commands here explaining what to do with A&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, we will print the first line. Thus we should have a program like this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;Let A be the following text:
&amp;lt;there are going to be more lines here&amp;gt;&quot;
Print the first line of A
&amp;lt;there are going to be more commands here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Obviously the command &lt;code class=&quot;highlighter-rouge&quot;&gt;Print the first line of A&lt;/code&gt; should appear somewhere in A. So let’s add it to A to obtain&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;Let A be the following text:
Print the first line of A
&amp;lt;there are going to be more lines here&amp;gt;&quot;
Print the first line of A
&amp;lt;there are going to be more commands here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now the second printing command should print what comes after the first line. But this is simply the text A in quotation. Therefore we should have&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;Let A be the following text:
Print the first line of A
&amp;lt;there are going to be more lines here&amp;gt;&quot;
Print the first line of A
Print A in quotation
&amp;lt;there are going to be more commands here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again these commands should appear in A. So we have&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;Let A be the following text:
Print the first line of A
Print A in quotaion
&amp;lt;there are going to be more lines here&amp;gt;&quot;
Print the first line of A
Print A in quotation
&amp;lt;there are going to be more commands here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that, up untill now, all the steps we took were pretty much forced. The final step will be a little different and require a tid bit of creativity.
Here is our finished quine:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be the following text:
&quot;Let A be the following text:
Print the first line of A
Print A in quotaion
Print A except for its first line&quot;
Print the first line of A
Print A in quotation
Print A except for its first line
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As an exercise, you may translate this quine into a real programming language. Even though the quine above has the essential idea, you may still need to deal with a few language specific details such as escaping quotation marks. Here is one for you in Haskell meant to be evaluated in repl.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let x = [&quot;let x = &quot;, &quot; in putStr (x !! 0) &amp;gt;&amp;gt; putStr (show x) &amp;gt;&amp;gt; putStr (x !! 1)&quot;] in putStr (x !! 0) &amp;gt;&amp;gt; putStr (show x) &amp;gt;&amp;gt; putStr (x !! 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now this was fun, bu also ad-hoc. The natural question to ask here whether there is a principled way of writing, not only quines, but programs which have some kind of access to their own source code. The answer is yes and this follows from one of the most fundamental results in recursion theory, the recursion theorem.&lt;/p&gt;

&lt;h1 id=&quot;kleenes-recursion-theorem&quot;&gt;Kleene’s Recursion Theorem&lt;/h1&gt;

&lt;p&gt;I want to work with a Turing complete programming language. Luckily, what we need is essentially what I described in the first section of
&lt;a href=&quot;https://sonatsuer.github.io/kolmogorov-complexity/2018/05/21/kolmogorov-complexity-1.html&quot;&gt;Kolmogorov Complexity (1/2)&lt;/a&gt;. The differences
are that we do not need \(\mathcal{L}\) to contain UTF-8 characters and we do not need an assumtion on the way we represent natural numbers.&lt;/p&gt;

&lt;p&gt;Let \(\mathcal{M}\) be the set of all strings from \(\mathcal{L}\) and let \(\mathcal{C}(n)\) be the set of all source codes
expecting \(n\) inputs, where \(n\) is a natural number. Then, for each \(c\in\mathcal{C}(n)\), we have a partial function \(f_c\) from
\(\mathcal{M}^n\) to \(\mathcal{M}\), which is simply the partial function computed by the source code \(c\). These functions are called the computable partial functions.&lt;/p&gt;

&lt;p&gt;Let us start with a simple but usefull lemma.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma:&lt;/strong&gt; There is a computable function \(s\) such that for any \(x\in\mathcal{M}\) and \(c\in\mathcal{C}(2)\) we have
\(s(c,x)\in\mathcal{C}(1)\) and
\[
   f_c(x,y) = f_{s(c,x)}(y).
\]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let \(x\in\mathcal{M}\) and \(c\in\mathcal{C}(2)\) be given. As \(c\in\mathcal{C}(2)\), \(c\) should look like this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ask for a value from the user and store it as A
Ask for a value from the user and store it as B
&amp;lt;Some commands&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;All \(s\) will need to do is to convert this code into the following one:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let A be x
Ask for a value from the user and store it as B
&amp;lt;Some commands&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In other words, it should hard-code the value \(x\). It is not difficult to imagine an algorithm doing this. Thus, by appealing to the Church-Turing hypothesis, we are done. \(\square\)&lt;/p&gt;

&lt;p&gt;Now we can prove the recursion theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theprem (Kleene):&lt;/strong&gt; Let \(f(x,y)\) be a partial two-variable computable function. Then there is a \(c\in\mathcal{C}(1)\) such that
\[
  f(c,y) = f_c(y)
\]
holds for all \(y\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let \(a\in\mathcal{C}(2)\) be such that \(f = f_a\). Such an \(a\) exists because this is what computable means. Now,
by the lemma, we already have something close, namely
\[
  f(x,y) = f_a(x,y) = f_{s(a,x)}(y).
\]
So, if we could find a solution for \(x = s(a,x)\), then we would be done. However, in this equation, we have some control over \(x\) but
the other parameter \(a\) is very heavily contsrined since \(f = f_a\).&lt;/p&gt;

&lt;p&gt;Now comes the brilliant idea: instead of working with \(x\)
let us work with a computable function of \(x\). Consider \(f(g(x),y)\) for a not yet determined computable function. As both \(f\)
and \(g\) are computable, so is this new function. Therefore there is a \(b\in\mathcal{C}(2)\) such that \(f_b(x,y) = f(g(x),y)\). Now we have
\[
  f(g(x),y) = f_b(x,y) = f_{s(b,x)}(y)
\] so the equation we need to solve in this case is
\[
  g(x) = s(b, x)
\]
instead of \(x = s(a,x)\). Introducing \(g\) into the problem gave us some room because now we can choose both \(x\) &lt;em&gt;and&lt;/em&gt; \(g\). Squinting
at the equation for a few seconds, we see that there is a trivial solution, namely \(g(x) = s(x,x)\) and \(x = b\)! We were lucky! (Just kidding,
I planned all of this.)&lt;/p&gt;

&lt;p&gt;Let us summarize this “stream of consciousness” into a more traditional proof. Let \(b\in\mathcal{C}(2)\) be such that \(f_b(x,y) = f(s(x,x),y)\)
and let \(c = s(b,b)\). Then
\begin{align}
f(c,y) &amp;amp; = f (s (b,b), y) \newline
       &amp;amp; = f_b (b, y) \newline
       &amp;amp; = f_{s(b,b)}(y) \newline
       &amp;amp; = f_c(y)
\end{align}
This finishes the proof. \(\square\)&lt;/p&gt;

&lt;p&gt;Let us look at example to appreciate the power of this theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; There is a \(c\in\mathcal{C}(1)\) such \(f_c(y) = c\) for all \(y\). Note that \(c\) is just a quine.
Let \(f(x,y) = x\). Obviously \(f\) is computable. Thus, by the recursion theorem, there is a \(c\) such that
\[
  f_c(y) = f(c, y) = c
\]
for all \(y\).&lt;/p&gt;

&lt;p&gt;Here is another one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; There is a \(c\in\mathcal{C}(1)\) such
\[
  f_c(y) =
  \begin{cases}
   \text{reversed } c, \text{ if $c$ is longer than $y$} \newline
   \text{collected works of Shakespeare}, \text{ otherwise}
  \end{cases}
\]
Let
\[
  f(x,y) =
  \begin{cases}
   \text{reversed } x, \text{ if $x$ is longer than $y$} \newline
   \text{collected works of Shakespeare}, \text{ otherwise}
  \end{cases}
\]
and apply the recursion theorem.&lt;/p&gt;

&lt;p&gt;The pattern here is clear: If you want to find a code in \(\mathcal{C}(1)\) which accesses its own source code, just write a code in
\(\mathcal{C}(2)\) which asks for its own source code as input from the user. The recurion theorem takes care of the rest. From now on, I will freely use a subroutine &lt;code class=&quot;highlighter-rouge&quot;&gt;access your own code&lt;/code&gt; and assume that you can implement it using the recursion theorem.&lt;/p&gt;

&lt;h1 id=&quot;unsolvable-problems&quot;&gt;Unsolvable Problems&lt;/h1&gt;

&lt;p&gt;Now we know how to fake an access-your-own-code subroutine. You might think that we can write amazing programs by using it but from a practical point of view such a subroutine is pretty useless. So what can we do with it? Well, mathematical logic of course! (See the last paragraf of &lt;a href=&quot;https://sonatsuer.github.io/kolmogorov-complexity/2018/05/21/kolmogorov-complexity-1.html&quot;&gt;Kolmogorov Complexity (1/2)&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;As a warmup exercise, let us prove the unsolvability of the halting problem using the recursion theorem. Of course there is a much simpler proof via diagonalization but, as I said, this is a warmup exercise. Suppose that the halting problem &lt;em&gt;is&lt;/em&gt; solvable. This means that the function defined by
\[
  f(c, x) =
  \begin{cases}
  \text{yes, }\, \text{if $c$ is in $\mathcal{C}(1)$ and $f_c(x)$ is defined} \newline
  \text{no, }\, \text{otherwise}
  \end{cases}
\]
is computable. Now consider the following algorithm:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ask for a value from the user and store it as X
Access your own code and call it S
if f(S, X) = no
  print &quot;I am not supposed to halt!&quot;
  Halt
if f(S, X) = yes
  print &quot;I am supposed to halt!&quot;
  loop forever
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This program looks at its code, and sees —using the function \(f\)— whether it will halt, and then does exactly the opposite. So it halts if and only if it does not halt! Contradiction.&lt;/p&gt;

&lt;p&gt;Before proving another impossibility result, let we will prove a fixed point version the recursion theorem, sometimes called the second recursion theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; Let \(g\) be a total computable function with a single variable such that \(g(c)\in\mathcal{C}(1)\) for all \(c\in\mathcal{C}(1)\). Then there is a \(S\in\mathcal{C}(1)\) such that \(f_S = f_{g(S)}\).&lt;/p&gt;

&lt;p&gt;Such an \(S\) is called a fixed point of \(g\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Consider the following code:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ask for a value from the user and store it as X
Access your own code and call it S
Simulate g(S) running on input X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let us call this code &lt;code class=&quot;highlighter-rouge&quot;&gt;S&lt;/code&gt;, just like the program itself does. Now for any given &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;, if we run &lt;code class=&quot;highlighter-rouge&quot;&gt;S&lt;/code&gt; on input &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;, we get what &lt;code class=&quot;highlighter-rouge&quot;&gt;g(S)&lt;/code&gt; does on input &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;. But this simply means that on any
&lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;, the codes &lt;code class=&quot;highlighter-rouge&quot;&gt;S&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;g(S)&lt;/code&gt; have the same behaviour. So we are done. \(\square\)&lt;/p&gt;

&lt;p&gt;Now we can prove a classical result due to Henry Gordon Rice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; Let \(\mathcal{A}\) be a decidable subset of \(\mathcal{C}(1)\) such that \(\mathcal{A}\not= \emptyset, \mathcal{C}(1)\). Then there are \(a,b\in\mathcal{C}(1)\) such that \(a\in \mathcal{A}\), \(b\in \mathcal{C}(1)\setminus\mathcal{A}\) and \(f_a = f_b\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Fix \(a’\in \mathcal{A}\), \(b’\in \mathcal{C}(1)\setminus\mathcal{A}\). Define
\[
  g(c) =
  \begin{cases}
  a’\text{ if } c\in\mathcal{A} \newline
  b’\text{ if } c\not\in\mathcal{A}
  \end{cases}
\]
As \(\mathcal{A}\) is decidable, \(g\) is computable. Let \(c\) be a fixed point of \(g\). Then \(f_c = f_{g(c)}\). Morover, by construction, \(c\) and \(g(c)\) cannot be both in \(\mathcal{A}\). This finishes the proof. \(\square\)&lt;/p&gt;

&lt;p&gt;This is a remarkable theorem, because it says that there is no decidable semantic property! For instance the following sets are all undecidable:
\[
  \begin{align}
  &amp;amp; \{c\in\mathcal{C}(1) | \text{domain of $f_c$ has 73 elements} \}, \newline
  &amp;amp; \{c\in\mathcal{C}(1) | \text{$f_c$ is total} \}, \newline
  &amp;amp; \{c\in\mathcal{C}(1) | \text{$f_c$ turns an even-length string to an odd-length string} \}, \newline
  &amp;amp; \{c\in\mathcal{C}(1) | \text{$f_c$ is constant} \}, \newline
  &amp;amp; \{c\in\mathcal{C}(1) | \text{$f_c(x)$ has the characters a and 7 for all $x$} \}, \newline
  &amp;amp; \{c\in\mathcal{C}(1) | \text{$f_c(x)$ is longer than $x$ for all $x$} \}, \newline
  &amp;amp; \ldots
  \end{align}
\]&lt;/p&gt;

&lt;p&gt;For our final example, we need a definition. Call an element \(c\in\mathcal{C}(1)\) minimal if for any shorter \(c’\in\mathcal{C}(1)\) we have \(f_c \not= f_{c’}\). It is not difficult to prove that minimality is not a decidable property. I will leave the details to you.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hint:&lt;/em&gt; Again, suppose that minimality &lt;em&gt;is&lt;/em&gt; decidable and consider the following code:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ask for a value from the user and store it as X
Access your own code and call it S
Construct a minimal M longer than S and call it W
Simulate W running on input X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Kolmogorov Complexity (Part 2 of 2)</title>
   <link href="http://localhost:4000/kolmogorov-complexity/2018/06/14/kolmogorov-complexity-2.html"/>
   <updated>2018-06-14T00:00:00+03:00</updated>
   <id>http://localhost:4000/kolmogorov-complexity/2018/06/14/kolmogorov-complexity-2</id>
   <content type="html">&lt;h1 id=&quot;a-crash-course-in-logic&quot;&gt;A Crash Course in Logic&lt;/h1&gt;

&lt;p&gt;In this post we wil apply th Kolmogorov complexity to logic. To be more precise, we will answer to classical questions about foundations
of mathematics. But first, let’s clarify what we mean by foundations.&lt;/p&gt;

&lt;p&gt;For simplicity and definiteness we will stick with set theory (with first order logic) as our foundational theory.
The idea is to express &lt;em&gt;every&lt;/em&gt; notion we use in mathematics in terms of sets using a formal language. Our formal language will have only two
primitive notions, namely equality, which we will denote by \(=\) and membership, which we will denote by \(\in\). Both equality and
memebership will be binary relations on sets.&lt;/p&gt;

&lt;p&gt;Other than these two, we will have the following symbols in our language:
\[
  \forall,\, \exists,\, \wedge,\, \vee,\, \neg,\, \rightarrow,\, \leftrightarrow,\, (,\, ),\, x,\, ‘.
\]&lt;/p&gt;

&lt;p&gt;The last symbol, namely \(‘\) is used to obtain an unbounded number of variables from the single variable symbol \(x\)
as follows:
\[
  x’,\, x’’\, x’’’\, x’’’’\, x’’’’’\, x’’’’’’\, \ldots
\]
However, for ease of reading, we will use lower case Roman letters such as \(x,y,z,t,\ldots\) instead of \(x’,\, x’’\, x’’’,\ldots\) The point
here is that even though our alphabet has finitely many symbols, we can talk about infinitely many variables.&lt;/p&gt;

&lt;p&gt;When we talk about sets, we will use only &lt;em&gt;meaningful&lt;/em&gt; or well formed strings in these symbols. I will not define what exactly well formed means but
you can assume that a well formed formula is a string in our symbols which can be parsed according to some grammar. Intuitively, strings like
\[
  \forall )( \rightarrow xx \;\;\text{ or }\;\; ‘‘\vee x \wedge
\]
are not well formed but
\[
  \forall x \forall y (( \forall t (t\in x)\leftrightarrow (t\in y)) \leftrightarrow x = y)
\]
is well formed. Actually this second formula says that two sets are equal if and only if they have the same elements. Note that menaingful
does not mean true. For instance
\[
  \exists x\forall y (y\in x)
\]
is meaningfull and says that there is set which contains all sets. However, it is not true in most set theories.&lt;/p&gt;

&lt;p&gt;One important property of well formed formulas is that there is an algorithm which can decide whether an arbitrary string is well formed
or not.&lt;/p&gt;

&lt;p&gt;We want to be able to prove statements about mathematical using our language. After all, this is supposed to be a foundational theory. Since
we have statements, namely the well formed formulas, only two pieces are missing now: assumptions (or axioms) and a notion of proof.&lt;/p&gt;

&lt;p&gt;Again for definiteness, we will work with the axioms of ZFC (Zermalo-Frenkel-Choice) set theory. The actual axioms are not important but you can look
them up if you are curious. The important fact about the axioms is the following: There is an algorithm which can decide whether an arbitrary string
is an axiom.&lt;/p&gt;

&lt;p&gt;Finally a proof of a well formed formula \(varphi\) is simply a sequence of well formed formulas \(\varphi_1,\varphi_2,\ldots,\varphi_n\) such that
\(\varphi_n = \varphi\) and each \(\varphi_i\) is either an axiom or it is obtained from earlier elements in the sequence using deduction rules.
Deduction rules here are rules like “from \(\varphi\rightarrow\psi\) and \(\varphi\) deduce \(\psi\)”. You guessed it, the actual rules are not
importatnt but the important fact about proofs is that there is algorithm which can decide whether an arbitrary sequence of strings is a proof or not.&lt;/p&gt;

&lt;p&gt;A theorem of ZFC is a well formed formula which has a proof. Now you might think that there is an algorithm which can decide whether an arbitrary string is a theorem of ZFC. However, as we will see soon, this is not true. The best you can do is to &lt;em&gt;enumerate&lt;/em&gt; all theorems of ZFC. In other words
there is an algorithm which generates an infinite list containing &lt;em&gt;all&lt;/em&gt; theorems of ZFC. You can probably guess the algorithm: Generate all finite
sequences of well formed formulas in, say, lexicographic ordering. If the sequence happens to be a proof, add the proven formula to the list.&lt;/p&gt;

&lt;p&gt;If you have not heard of recursively enumerable sets before, you might want to think about why this is weaker than having an alogorithm which can
decide whether an arbitrary string is a theorem.&lt;/p&gt;

&lt;h1 id=&quot;chaitins-incompleteness-theorem&quot;&gt;Chaitin’s Incompleteness Theorem&lt;/h1&gt;
&lt;p&gt;At this point, you might ask the following question: We have only talked about sets so far but we were looking for a foundational theory
for all of mathemtaics. Is this really enough? The answer is yes. This may look strange because it seems that there objects in mathematics which are
clearly not sets. For instance \(\pi\) is not a set, it is a number. The trick is that \(pi\), and all mathematical objects for that matter, can
be &lt;em&gt;encoded&lt;/em&gt; as sets.&lt;/p&gt;

&lt;p&gt;Here are a few examples to give you an idea. We can define
\[
  0 = \emptyset,\; 1 = \{0\},\; 2 = \{0,1\},\; 3 = \{0,1,2\},\ldots
\]
So a natural number is the set of smaller natural numbers. For two sets \(x\) and \(y\) we can define
\[
  (x,y) = \{ \{x\}, \{x,y\}\}
\]
and prove that this actually works like an ordered pair (Exercise: Do it!). Once you have ordered pairs, you can have Cartesian products, relations and
functions etc.&lt;/p&gt;

&lt;p&gt;From now on I will assume that we agreed on encodings of all the notions in mathematics as sets. Note that notions like well formed formula, proof and
algorithm are also encoded in this way. So ZFC is capable of talking about Kolmogorov complexity, or even itself.&lt;/p&gt;

&lt;p&gt;Here comes the big theorem of this post. Recall that \(K\) denotes the Kolmogorov complexity.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;There is a constant \(L\) such that for no \(\sigma\), the statement \(K(\sigma) &amp;gt; L\) is a theorem of ZFC.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Suppose not. Then for any natural number \(n\) there is a \(\sigma_n\) such that \(K(\sigma_n)\ &amp;gt; n\) is a theorem of ZFC. This implies that
there is a computable function \(f\) such that for each natural number \(n\) the inequality \(K(f(n)) &amp;gt; n\) is a theorem of ZFC. Indeed,
given any \(n\), using the fact that theorems of ZFC are enumerable, search for a theorem of the form \(K(\sigma) &amp;gt; n\). By assumption this
search will end even though the list is infinite. Define \(f(n)\) to be the first such \(\sigma\) you find. But this is the first step in
the argument which we proves the incomputability of \(K\) in the previous post! You can simply copy the rest of the proof here.&lt;/p&gt;

&lt;p&gt;Let us stop here and look at what we have proved. In ZFC, there is an upper bound on the &lt;em&gt;provable&lt;/em&gt; Kolmogorov complexity. However, there is
no upper bound on Kolmogorov complexity! Therefore there are some true statemets of the form \(K(\sigma) &amp;gt; k\) which are not theorems of ZFC.
This means that ZFC is incomplete, in the sense that it cannot capture all true statemets. This is Chaitin’s version of Gödel’s first incompleteness
theorem.&lt;/p&gt;

&lt;h1 id=&quot;gödels-second-incompleteness-theorem&quot;&gt;Gödel’s Second Incompleteness Theorem&lt;/h1&gt;
&lt;p&gt;We have been making an implicit assumtion about our foundational theory, namely that it was consistent. In other words, we assumed that no
contradiction is a theorem of ZFC. A contraiction is a vacuously false statment like \(\exists x (\neg x = x)\), by the way. This is equivalent to
assuming that not all well formed formulas are theorems of ZFC as everything can be deduced from a contradiction. Oviously, this is a must have
for a foundational theory.&lt;/p&gt;

&lt;p&gt;Recall that ZFC can prove statements about itself. So wouldn’t it be nice to actaully have a proof of this assumption in ZFC? This would be proving the consistency of our foundational theory within the theory itself, showing that the theory is self sufficient. Certainly Hilbert wanted to do it.
However his efforts were crashed by, Gödel who proved that no sufficiently rich theory can prove its own consistency. Here sufficiently rich roughly
means strong enough to interpret arithmetic. Now we will give a modern proof of this theorem using Kolmogorov complexity. The proof is due to Shira
Kritchman and Ran Raz.&lt;/p&gt;

&lt;p&gt;Their proof is loosely based on the surprise examination paradox:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In your introduction to logic class, you announce that the students will have an exam next week, but they will not
know the exact day of the exam. So the exam cannot be on Friday because otherwise, the students will know that the exam
will be on Friday after not having an exam on Thursday. Similarly the exam cannot be on Thursday, Wednesday, Tuesday
or Monday.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let \(L\) be as in Chaitin’s theorem. Let \(\ell = c^{L + 1}\) where \(c\geq 2\) is the number of symbols we use in our fixed
programming language. Define
\[
  \mathcal{K} = \{\sigma\, \colon K(\sigma) &amp;gt; L,\, \ell(\sigma) \leq \ell \}
  \;\;\text{ and }\;\;
  k = |\mathcal{K}|
\]
The number \(k\) will roughly correspond to the day on which the exam will take place counted from the end of the week. Clearly \(k\geq 1\) as
there are less than \(\ell\) many programs of size \(L\). Moreover, this observation is also a theorem of ZFC as it is simple counting.&lt;/p&gt;

&lt;p&gt;Now let us prove that \(k \geq 2\). Suppose not. Then \(k=1\). Let \(\sigma_0\) be the unique element of \(\mathcal{K}\). Using the fact that the theorems of ZFC are enumerable, look for proofs of statements of the form \(K(\sigma) \leq L\) for \(\ell(\sigma) \leq \ell\). By assumption, for all \(\sigma\not=\sigma_0\) we have such a proof. So when we have \(\ell - 1\) proofs, the remaining string should be \(\sigma_0\) which has
to have complexity greater than \(L\). But we just proved, in ZFC, that a certain string has complexity greater than \(L\). This contradicts the choice of \(L\).&lt;/p&gt;

&lt;p&gt;Now let us prove that \(k \geq 3\). Suppose not. Then \(k = 2\). Let \(\sigma_0, \sigma_1\) be the only elements of \(\mathcal{K}\). Using the
fact that the theorems of ZFC are enumerable, look for proofs of statements of the form \(K(\sigma) \leq L\) for \(\ell(\sigma) \leq \ell\). By assumption, for all \(\sigma\not=\sigma_0, \sigma_1\) we have such a proof. So when we have \(\ell - 2\) proofs, the remaining two strings should
be \(\sigma_0\) and \(\sigma_1 \) which have to have complexity greater than \(L\). But we just proved, in ZFC, that a certain string has complexity greater than \(L\). This contradicts the choice of \(L\).&lt;/p&gt;

&lt;p&gt;Now let us prove that \(m \geq 4\)\ldots Well, you see the pattern. Using this method we can exhaust all possible values for \(k\) and obtain a
contradiction. So assuming that ZFC can prove its consistency leads to a contrdiction.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Kolmogorov Complexity (Part 1 of 2)</title>
   <link href="http://localhost:4000/kolmogorov-complexity/2018/05/21/kolmogorov-complexity-1.html"/>
   <updated>2018-05-21T00:00:00+03:00</updated>
   <id>http://localhost:4000/kolmogorov-complexity/2018/05/21/kolmogorov-complexity-1</id>
   <content type="html">&lt;h1 id=&quot;a-mandatory-example&quot;&gt;A Mandatory Example&lt;/h1&gt;

&lt;p&gt;Consider the following two binary strings:
\begin{align}
&amp;amp;\texttt{0101010101010101010101010101010},\newline
&amp;amp;\texttt{1100100100001111110110101010001}.
\end{align}&lt;/p&gt;

&lt;p&gt;Which one do you think is easier to remember? Most people would say that the first one, because
there is an obvious and simple rule/method to generate it. So, instead of remembering the string,
we can remember the rule. One can also say that the first string is simple while the second one is
complex. Kolmogorov complexity is a quantified version of this intuitive notion.&lt;/p&gt;

&lt;p&gt;Let us semi-formalize it. First let us fix a Turing complete programming language. I will not define what a programming
language &lt;em&gt;is&lt;/em&gt; or &lt;em&gt;should be&lt;/em&gt; and, frankly, be somewhat vague about it. Hence the prefix &lt;em&gt;semi&lt;/em&gt; in front of
&lt;em&gt;formalize&lt;/em&gt;. Though I will make a few assumptions. Programs in our programming language will be strings
from from a fixed alphabet \(\mathcal{L}\), say, containing ASCII or UTF-8. I will also assume that each program
accepts strings from \(\mathcal{L}\) as input and produce strings from \(\mathcal{L}\) as output. So,
in particular, we can give a program to another program as input or a program may produce a program. For ease
of reading, I will use \(\texttt{this font}\) when writing a string from \(\mathcal{L}\).&lt;/p&gt;

&lt;p&gt;One technical assumption which will be important later is that our programs will use decimal representation
for natural numbers. Actually pretty much any positional system would do, but for the sake of definiteness
I will stick with decimal.&lt;/p&gt;

&lt;p&gt;When I write a program, I will use some sort of pseudocode hoping that there is no ambiguity. Also I will
not exclude partial programs which corresponds do partial functions. For instance
\begin{align}
&amp;amp;\texttt{accept x as input}\newline
&amp;amp;\texttt{while 1 == 1}\newline
&amp;amp;\;\;\;\texttt{do nothing}\newline
&amp;amp;\texttt{return x}
\end{align}
is a valid program even though it does not produce any result as it is always stuck in the while loop.&lt;/p&gt;

&lt;p&gt;Now we can define the Kolmogorov complexity of a string from \(\mathcal{L}\): Let \(\sigma\) be a string
from \(\mathcal{L}\). The Kolmogorov complexity of \(\sigma\) is the length of the shortest program which
generates \(\sigma\) on any input. It is denoted by \(K(\sigma)\).&lt;/p&gt;

&lt;h1 id=&quot;how-canonical-is-this-definition&quot;&gt;How canonical is this definition?&lt;/h1&gt;

&lt;p&gt;The definition seems very intuitive, especially after the mandatory example. However, there is something fishy here.
We made several arbitrary choices yet we called \(K(\sigma)\) &lt;em&gt;the&lt;/em&gt; Kolmogorov complexity of \(\sigma\). It should
be clear that it is easier to generate certain strings in some languages than others. Indeed, the second string in our
example is just the first few digits of the binary expansion of \(\pi\) so it can be easier to generate in a language
designed for numerical computations.&lt;/p&gt;

&lt;p&gt;Even worse, for any string \(\sigma\), we can very well imagine a programming language which has \(\sigma\) as a built-in
constant. Thus the Kolmogorov complexity of a single string seems to very heavily depend on our language of choice. So
does this mean that our definition of \(K\) is arbitrary? What happens if we change our programming language?&lt;/p&gt;

&lt;p&gt;Let us consider two notions of complexity \(K_1\), \(K_2\) corresponding
to two different programming languages \(L_1\) and \(L_2\), respectively. Since \(L_1\) is Turing complete, we can write a
&lt;em&gt;compiler&lt;/em&gt; for \(L_2\) in \(L_1\). In other words, \(L_1\) can simulate \(L_2\). Let \(\sigma\) be a string and
let \(n=K_2(\sigma)\). Then there is a program \(p\) in the language \(L_2\) which witnesses the fact that \(n=K_2(\sigma)\).
That is, \(p\) has length \(n\) and \(p\) generates \(\sigma\). Now consider the following (description of a) program in \(L_1\):
Simulate \(p\). The length of this program will be \(n + c\) for some \(c\) because the program contains the string \(p\) and
\(p\) has length \(n\). The part corresponding to \(c\) is the part that simulates \(L_2\) and it does not depend on \(\sigma\).
By construction, “Simulate \(p\)” generates \(\sigma\), thus
\[
  K_1(\sigma)\leq n + c = K_2(\sigma) + c.
\]
We proved that there is a constant \(c\) such that \(K_1(\sigma)\leq K_2(\sigma) + c\) for all \(\sigma\). By symmetry,
there is a \(c’\) such that \(K_2(\sigma)\leq K_1(\sigma) + c’\) for all \(\sigma\). Combining these two we obtain
\[
  |K_1(\sigma) - K_2(\sigma)| &amp;lt; {\rm max}\{c, c’\}.
\]
This shows that if we consider an infinite family of strings and consider the asymptotic behavior of Kolmogorov complexity
on that family, then the programming language we choose does not matter. Obviously, this does not imply that this asymptotic
behavior is easy to determine.&lt;/p&gt;

&lt;p&gt;From now on we will stick with a fixed but not explicitly determined choice of language and denote the complexity function
given by that language by \(K\).&lt;/p&gt;

&lt;p&gt;At this point I would compute the Kolmogorov complexity of some concrete strings (or infinite families of strings) but it is tricky.
We can always give an upper bound for Kolmogorov complexity by explicitly constructing a program and measuring its length
but the tricky part is to show that no shorter program generates the same string. Actually, the problem is so difficult that
there is no general solution. To put it more concretely, the function  \(K\) is not computable. The aim of this post is to
give a proof of this result.&lt;/p&gt;

&lt;h1 id=&quot;berry-paradox&quot;&gt;Berry Paradox&lt;/h1&gt;

&lt;p&gt;Before moving on to the actual proof, let us see the underlying idea of the proof in a linguistic context. Let us consider the following
description of a number:&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp;\texttt{the least natural number that cannot be}\newline
&amp;amp;\texttt{described in English by less than 88 characters}
\end{align}&lt;/p&gt;

&lt;p&gt;Let \(n\) be the natural number defined by this description. But this description itself has only 87 characters, counting
spaces and digits, so \(n\) has a description with only 87 characters. Contradiction! This argument is known as the Berry
paradox. I will not go into a linguistic or philosophical debate here, there is already a substantial body of literature on the topic.&lt;/p&gt;

&lt;p&gt;Now note that if we start with the following description, we cannot really say much about the number it describes,
nevertheless, we do not end up with a paradox either:&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp;\texttt{the least natural number that cannot be}\newline
&amp;amp;\texttt{described in English by less than 75 characters}
\end{align}&lt;/p&gt;

&lt;p&gt;Thus the number in the description is not arbitrary. Here is a natural question: What values of \(n\) give rise to the Berry paradox?
To answer that question we will make the number \(n\) a parameter. For a natural number \(n\) let \(\ulcorner n \urcorner\) be the
decimal expression of \(n\) as a string. So, for instance \(\ulcorner 234 \urcorner = \texttt{234}\), \(\ulcorner 0 \urcorner = \texttt{0}\),
etc. Now define \(\Delta(n)\) be&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp;\texttt{the least natural number that cannot be}\newline
&amp;amp;\texttt{described in English by less than $\ulcorner n \urcorner$ characters}
\end{align}&lt;/p&gt;

&lt;p&gt;Clearly a positive natural number \(n\) gives rise to the Berry paradox if \(\ell(\Delta(n)) &amp;lt; n\) where \(\ell(\sigma)\)
denotes the length of \(\sigma\). Note that
\[
  \ell(\Delta(n)) = 85 + \ell(\ulcorner n \urcorner) = 85 + 1 + \lfloor \log_{10} n \rfloor.
\]
because \(\ell(\ulcorner n \urcorner)\) is simply the number of digits of \(n\). So the inequality we should solve is
\[
  86 + \lfloor \log_{10} n  \rfloor &amp;lt; n.
\]
We can of course find the exact solution set of this inequality but what is more interesting is that we can immediately tell
that there are solutions. The reason is that the left hand side grows logartihmically and the right hand side grows linearly and hence
the right hand side should dominate the left hand side for all sufficiently large values of \(n\). Even better, if we change the
phrasing of the condition or express it in a different language such as  french, we would need to solve the equation \(c + \lfloor \log_{10} n  \rfloor &amp;lt; n\) for some constant \(c\) and, by the same argument, we would have a solution. Thus the Berry paradox is about exploiting the fact that our measure
of complexity can be referred to in a not so complex way. Note that if we were to use the unary system instead of decimal, that is if we expressed \(n\) as \(n\)-many \(\texttt{1}\)’s, then we would have \(\ell(\Delta(n))=c + n\) for some \(n\) and the Berry paradox would not come up.&lt;/p&gt;

&lt;h1 id=&quot;an-incomputability-result&quot;&gt;An Incomputability Result&lt;/h1&gt;
&lt;p&gt;Here is the theorem we: There is no computer program which takes \(\sigma\) as input and produces \(\ulcorner K(\sigma) \urcorner\)
as output. For the proof, we will mimic the Berry paradox.&lt;/p&gt;

&lt;p&gt;Suppose that there &lt;em&gt;is&lt;/em&gt; such a program \(p\). We will obtain a contradiction. Consider the following program:
\begin{align}
&amp;amp;\texttt{accept n as input}\newline
&amp;amp;\texttt{if n does not represent a natural number then halt}\newline
&amp;amp;\texttt{set x to be the first string in alphabetical order}\newline
&amp;amp;\texttt{while the Kolmogorov complexity of x is less than n}\newline
&amp;amp;\;\;\;\texttt{replace x by the next string in alphabetical order}\newline
&amp;amp;\texttt{return x}
\end{align}&lt;/p&gt;

&lt;p&gt;Note that we use \(p\) to check the condition in the while loop. Also note that the while loop is never stuck because
there is no bound on the Kolmogorov complexity. So this program produces a string of complexity grater than \(n\) if its
input is \(\ulcorner n \urcorner\).&lt;/p&gt;

&lt;p&gt;Now for each \(n\) we can construct a program \(\tau_n\) as follows:&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp;\texttt{accept u as input}\newline
&amp;amp;\texttt{set x to be the first string in alphabetical order}\newline
&amp;amp;\texttt{while the Kolmogorov complexity of x is less than $\ulcorner n\urcorner$}\newline
&amp;amp;\;\;\;\texttt{replace x by the next string in alphabetical order}\newline
&amp;amp;\texttt{return x}
\end{align}&lt;/p&gt;

&lt;p&gt;Clearly \(\tau_n\) ignores the input and behaves like \(\tau\) with input \(\ulcorner n\urcorner\). Moreover
\(\ell(\tau_n) = \lfloor\log_{10}(n)\rfloor + c\) for some constant \(c\). Thus, for a sufficiently large \(k\) we have \(\ell(\tau_k) &amp;lt; k\).&lt;/p&gt;

&lt;p&gt;Let \(\omega\) be the string produced by \(\tau\) on an input \(\ulcorner k \urcorner\) satisfying \(\ell(\tau_k) &amp;lt; k\). Here comes the finishing blow: By the construction of \(\tau\), we have \(K(\omega)\geq k\). On the other hand \(\tau_k\) also produces \(\omega\) (on any input) therefore we must have \(K(\omega)\leq \ell(\tau_k)\). But these two inequalities imply that \(k \leq \ell(\tau_k)\). Contradiction!&lt;/p&gt;

&lt;p&gt;Let us summarize what we did: We defined a not exactly canonical notion of complexity which is impossible to compute in practice. So what can we even do with it? Well, mathematical logic of course! This will be the topic of the forthcoming post on Kolmogorov complexity.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Curry-Howard Correspondence From Scratch (Part 2 of 2)</title>
   <link href="http://localhost:4000/curry-howard/2018/05/14/curry-howard-from-scratch-2.html"/>
   <updated>2018-05-14T00:00:00+03:00</updated>
   <id>http://localhost:4000/curry-howard/2018/05/14/curry-howard-from-scratch-2</id>
   <content type="html">&lt;h1 id=&quot;simply-typed-lambda-calculus&quot;&gt;Simply Typed Lambda Calculus&lt;/h1&gt;

&lt;p&gt;In the first post we constructed a formal theory of propositions but left the
notion of &lt;em&gt;naming a specific element&lt;/em&gt; from a function set vague. Now we are going to fix that.&lt;/p&gt;

&lt;p&gt;Our central notion will be that of a &lt;em&gt;lambda term&lt;/em&gt;. As we want to define a formal theory we will
not define what a lambda term is and focus on how to construct new lambda terms from old ones. Intuitively
though, we will think of lambda terms as function prototypes.&lt;/p&gt;

&lt;p&gt;First we need variables. We will assume that we have an infinite set of variables and denote
variables by lower case letters from the Latin alphabet. Each variable will also be a lambda term.
Other than that, there will be two ways to construct a lambda term. First, if \(M\) and \(N\) are
lambda terms then so is \(MN\). We will think of \(MN\) as \(M\) applied to \(N\). Second,
if \(x\) is a variable and \(M\) is a lambda term than so is \(\lambda x.M\). Intuitively
\(\lambda x.M\) will mean \(x\mapsto M(x)\). This will be clearer when we start using lambda terms.&lt;/p&gt;

&lt;p&gt;Let us look at a few examples. The following are all lambda terms constructed using variables only:
\[
  x,\; xy,\; xx,\; x(yz),\; (xy)z,\; (xx)x,\; (xy)(x(yz)).
\]
Of course we can also use \(\lambda\):
\[
  \lambda x.x,\; \lambda y. y,\; \lambda x. y,\; \lambda y. (\lambda x. (yx))),\;
  (\lambda x. (xy))(\lambda y. x).
\]
To reduce the number of brackets we will give application higher priority than \(\lambda\).
So, for instance, the expression \(\lambda x. yz\) will stand for \(\lambda x. (yz)\) rather than
\((\lambda x. y)z\).&lt;/p&gt;

&lt;p&gt;Now let us define when two lambda terms are equal. Since we think of \(\lambda x.M\) as a function
mapping \(x\) to \(M\), we see \(\lambda x.x\) as some kind of identity function. However, under
this interpretation, there should not be a difference between \(\lambda x.x\) and \(\lambda y.y\).
More generally there should not be a difference between \(x\mapsto M(x)\) and \(y\mapsto M(y)\).
Therefore we want
\[
  \lambda x. x  = \lambda y. y = \lambda z. z = \ldots
\]
Maybe slightly more interestingly, we also want
\begin{align}
\lambda z. ((\lambda x.x)(\lambda y.yz))   &amp;amp;=
\lambda u. ((\lambda x.x)(\lambda y.yu))\newline &amp;amp;=
\lambda z. ((\lambda x.x)(\lambda u.uz))\newline &amp;amp;=
\lambda z. ((\lambda z.z)(\lambda u.uz))\newline &amp;amp;=
\ldots
\end{align}&lt;/p&gt;

&lt;p&gt;We will call this &lt;em&gt;harmless renaming of variables&lt;/em&gt; \(\alpha\)-equivalence. One can define \(\alpha\)-equivalence
formally but it is slightly messy. Therefore we will not do it here and instead adopt a convention: In a \(\lambda\)
term, each \(\lambda\) will use a different variable. So, for instance, we will avoid using the last lambda term
in the list above since \(\lambda z\) appears twice in it.&lt;/p&gt;

&lt;p&gt;Now comes the more interesting properties of equality. We define
\[
  (\lambda x. M)N = M[x:= N].
\]
Here \(M\) is a lambda term and \( M[x:= N]\) stands for the lambda term obtained from \(M\) by replacing each
occurrence of \(x\) by \(N\). For instance
\[
  (\lambda x. x) N = x[x := N] = N.
\]
So \(\lambda x. x\) does behave like identity. Here is another example:
\begin{align}
(\lambda y. (\lambda x. yx))(\lambda z. z)  &amp;amp;= (\lambda x. yx)[y:=\lambda z. z] \newline
&amp;amp;= \lambda x. ((\lambda z.z)x) \newline
&amp;amp;= \lambda x. (z[z:=x]) \newline
&amp;amp;= \lambda x. x.
\end{align}&lt;/p&gt;

&lt;p&gt;Finally, let us express the functions \(C\) and \(E\) we defined in the previous post as lambda terms. By definition
\(C_x(y)=x\) so \(C_x = \lambda y. x\). If we view \(C_x\)  as a function of \(x\) then we get
\[
  C = \lambda x. (\lambda y. x).
\]
Again by definition we have \(E_x(f) = f(x)\) so \(E_x = \lambda f. f x\). Therefore
\[
  E = \lambda x. (\lambda f. f x).
\]&lt;/p&gt;

&lt;p&gt;What we defined so far is called the &lt;em&gt;untyped lambda calculus&lt;/em&gt;. Even though it gives us a formal theory of
functions, it is not enough for our purposes as we also need function sets. To be able to talk about some
sort of function sets, we will introduce the notion of &lt;em&gt;type&lt;/em&gt; into our theory.&lt;/p&gt;

&lt;p&gt;The definition of a type is going to be very simple: if \(\alpha\) and  \(\beta\) are types then so is
\(\alpha\to\beta\). Note that, formally speaking, there is no difference between types and propositions we defined in the
previous post. On the other hand, while a proposition has the connotation of a judgement, a type will be
more like function sets.&lt;/p&gt;

&lt;p&gt;Now we will define our typing relation between lambda terms and types. If a lambda term \(M\) is related to the type
\(\tau\) then we will say that \(M\) is of type \(\tau\) and denote it by \(M\colon\tau\). We will call
a statement like \(M\colon\tau\) a type assignment. A context will simply be a set of type assignments. Our typing system
will consist of rules, which given a context, allows us to derive type assignments. If a context \(\Gamma\) allows us
to derive an assignment \(M\colon\tau\) we will denote it by \(\Gamma\vdash M\colon\tau\). Note that we are overloading
the symbol \(\vdash\) here.&lt;/p&gt;

&lt;p&gt;Here is the first rule. Let \(\Gamma\) be a context, then
\[
  \Gamma,x\colon\tau\vdash x\colon\tau\
\]
So, if the type of variable \(x\) is \(\tau\) in a given context, then we can derive the assignment \(x\colon\tau\)
in that context. Let us call this the rule \(A^\to\).&lt;/p&gt;

&lt;p&gt;Here is the second rule:
\[
  \text{if }\;\,
  \Gamma\vdash N:\sigma\;\,
  \text{ and }\;\,\
  \Gamma\vdash M:\sigma\to\tau\;\;
  \text{ then }\;\;\Gamma\vdash MN:\tau.
\]
So if \(M\) is a function which maps objects of type \(\alpha\) to objects of type \(\beta\) and
\(N\) is of type \(\alpha\) then \(M\) applied to \(N\) should be of type \(\beta\). Let us call this
the rule \(B^\to\).&lt;/p&gt;

&lt;p&gt;Finally,
\[
  \text{if }\;\;\;
  \Gamma,x:\tau\vdash M:\sigma \;\;\;
  \text{ then }\;\;\;
  \Gamma\vdash \lambda x. M:\tau\to\sigma.
\]
If \(x\) is of type \(\tau\) then any lambda term starting with \(\lambda x.\) should be a function
from \(\tau\) to somewhere. As \(M\) is of type \(\sigma\), obviously we should have \(\lambda x. M:\tau\to\sigma\).
Let us call this the rule \(C^\to\).&lt;/p&gt;

&lt;p&gt;The system we constructed with rules \(A^\to\), \(B^\to\) and \(C^\to\) is called the simply typed
lambda calculus in the style of Church and sometimes denoted by \(\lambda^\to\). At this point you might
want to compare \({\rm IP}(\to)\) and \(\lambda^\to\), especially the rules \(A\), \(B\), \(C\)
and \(A^\to\), \(B^\to\), \(C^\to\).&lt;/p&gt;

&lt;p&gt;Now we can give a precise definition of naming a specific element from a function set. Let \(A\) be a function
set as we used in the previous post. Formally, we can view \(A\) as a type, say \(\tau\). If there is a
lambda term \(M\) such that \(\vdash M\colon\tau\) then we say that one can name a specific function from
\(A\), namely \(M\).&lt;/p&gt;

&lt;p&gt;As an example consider the lambda term \(\lambda x.(\lambda f.f x)\). Recall that it corresponds to
\(x\mapsto E_x\). Let \(\Gamma=\{x:\alpha,\,f:\alpha\to\beta\}\). Then, by the rule \(A^\to\), we
get
\[
  \Gamma\vdash x\colon\alpha \;\;
  \text{ and }\;\;
  \Gamma\vdash f\colon\alpha\to\beta.
\]
And from these, together with rule \(B^\to\) we obtain \(\Gamma\vdash f x: \beta\). Finally, applying
the rule \(C^\to\) twice we derive
\[
  \vdash \lambda x. (\lambda f. f x) : \alpha\to((\alpha\to\beta)\to\beta).
\]&lt;/p&gt;

&lt;h1 id=&quot;finally-the-curry-howard-correspondence&quot;&gt;Finally, The Curry-Howard Correspondence&lt;/h1&gt;
&lt;p&gt;We need one final definition before we can express the Curry-Howard correspondence between \({\rm IP}(\to)\) and
\(\lambda^\to\) formally.&lt;/p&gt;

&lt;p&gt;Let \(\Gamma\) be a context. Define
\[
  |\Gamma| =\{\tau \colon \text{ there is a variable $x$ such that } x : \tau\in\Gamma\}.
\]
Note that \(|\Gamma|\) consists of types/propositions. In other words, \(|\Gamma|\) is an assumption set. is the set of variables
appearing in \(\Gamma\).&lt;/p&gt;

&lt;p&gt;Finally, I present you the Curry-Howard Correspondence for \({\rm IP}(\to)\) and \(\lambda^\to\):&lt;/p&gt;

&lt;p&gt;If \(\Gamma\vdash M:\varphi\) then \(|\Gamma|\vdash\varphi\). Conversely, if \(|\Gamma|\vdash\varphi\) then
there is a lambda term \(M\) such that \(\Delta\vdash M:\varphi\) where \(\Delta=\{x_\psi:\psi\;|\; \psi\in\Gamma\}\).
To paraphrase it as a slogan, if we view types as propositions, provable propositions are precisely the types of
lambda terms.&lt;/p&gt;

&lt;p&gt;Even this modest version of the Curry-Howard correspondence is beautiful and surprising. One wonders if it is possible to
generalize it. Actually, there is a very natural direction of generalization. We know that \({\rm IP}(\to)\) is a restricted version
of a more general logic, namely the full intuitionistic propositional logic. So we can try to generalize the theorem
to different logics. However, it is not clear how to generalize \(\lambda^\to\), especially if you have not seen lambda calculus
before. The idea is that \(\lambda^\to\) is a programming language and in order to generalize the Curry-Howard correspondence, we need
to find/invent different programming languages with different type systems.&lt;/p&gt;

&lt;p&gt;First, let us clarify what we mean by \(\lambda^\to\) is a programming language. Let us fix a type \(\tau\) and define
\(\texttt{Nat}=(\tau\to\tau)\to(\tau\to\tau)\). As the name suggests, \(\texttt{Nat}\) will be the set of natural
numbers. For lambda terms \(M\) and \(N\) let us make the following recursive definition: \(M^0N = N\) and
\(M^{n+1}N=M(M^nN)\). So \(M^nN\) is \(M\) applied to \(N\), \(n\)-times. The Church encoding
of a natural number \(n\) is
\[
  \ulcorner n \urcorner = \lambda s. (\lambda z. s^n z).
\]
As you can check easily, \(\vdash \ulcorner n \urcorner : \texttt{Nat}\). We can also define basic operations on \(\texttt{Nat}\).
For instance if
\[
  A_+ = \lambda x. (\lambda y. (\lambda s. (\lambda z. (xs)((ys)z))))
\]
then we have \(\vdash A_+ : \texttt{Nat} \to (\texttt{Nat} \to \texttt{Nat})\) and for all natural number \(m\) and \(n\) the
equality \((A_+\ulcorner m \urcorner )\ulcorner n \urcorner =\ulcorner m + n \urcorner \) holds. Therefore we can interpret
\(A_+\) as a program which adds numbers. As an exercise you might want to define a computer program which does multiplication.&lt;/p&gt;

&lt;p&gt;One can even define booleans in \(\lambda^\to\). Let \({\bf T}=\lambda y. (\lambda x . x)\) and \({\bf F}=\lambda y. (\lambda x . y)\).
If we interpret \({\bf T}\) as true and \({\bf F}\) as false then \((BP)Q\) acts like
\[
  {\bf if}\; B\; {\bf then}\; P \;{\bf else}\; Q
\]
if \(B\) is \({\bf T}\) of \({\bf F}\). These may look like ad-hoc ideas but lambda terms (without their types) is actually a
universal way of writing programs. This is the topic of another post, though.&lt;/p&gt;

&lt;p&gt;Now back to the original question (with a finer statement): Can we find/invent a different programming language whose typing rules
corresponds to a logic in such a way that proving a proposition/type in that logic corresponds to writing a program of that
proposition/type? The answer is amazingly yes!&lt;/p&gt;

&lt;p&gt;For a very simple example, consider the &lt;em&gt;and&lt;/em&gt; operation on propositions.
The type construction rule corresponding to logical &lt;em&gt;and&lt;/em&gt; should be pairing, also known as the Cartesian product. Because, using the analogy
in the first post, naming an element from a set \(X\) &lt;em&gt;and&lt;/em&gt;  a set \(Y\) is the same as naming an element from \(X\times Y\).
In a similar fashion, &lt;em&gt;or&lt;/em&gt; corresponds to sum types. One can also consider logics with quantifiers, modalities or linear logics etc. and they
all come with their corresponding programming languages. Moreover the properties of the type system viewed as a logic are reflected as
programming language features such as polymorphism, staging, resource awareness etc. So what we covered in these two posts is just the tip
of the iceberg.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Curry-Howard Correspondence From Scratch (Part 1 of 2)</title>
   <link href="http://localhost:4000/curry-howard/2018/05/05/curry-howard-from-scratch-1.html"/>
   <updated>2018-05-05T00:00:00+03:00</updated>
   <id>http://localhost:4000/curry-howard/2018/05/05/curry-howard-from-scratch-1</id>
   <content type="html">&lt;h1 id=&quot;function-sets&quot;&gt;Function Sets&lt;/h1&gt;

&lt;p&gt;Suppose we have two sets \(X\) and \(Y\). Can we name a function from \(X\) to \(Y\) without
using what \(X\) or \(Y\) is? Obviously if \(Y\) is empty and \(X\) is not empty then this is not possible
as there is no function from a nonempty set to the empty set. So let us suppose \(X\) and \(Y\) are nonempty
for the moment. Now there &lt;em&gt;are&lt;/em&gt; functions from \(X\) to \(Y\). For instance, for every element \(y\in Y\),
there is the constant function which takes the value \(y\). Bu this does not count because the problem is to &lt;em&gt;name&lt;/em&gt; a
particular function, not to prove the existence of such functions. It looks difficult. Actually, in a sense
which we will define later, the answer is that we cannot name such a function. Considering the fact that we do not
know anything about \(X\)  and \(Y\) this is hardly surprising.&lt;/p&gt;

&lt;p&gt;On the other hand, if we take \(X=Y\) then it is possible to name such a function: the identity function. Since
the identity function maps each element to itself, we do not need to know anything about \(X\). Frankly, this
does not seem like an interesting observation, either. However, as we consider more complex examples, the situation will
change dramatically.&lt;/p&gt;

&lt;p&gt;Let us introduce some notation. For sets \(X\) and \(Y\), we will denote the set of functions from \(X\) to \(Y\)
by \(X\to Y\). In this notation \(f \in X\to Y\) and \(f \colon X \to Y\) have the same meaning. For instance,
if we denote the identity function on \(X\) by \(1_X\), then we have \(1_X \in X\to X\).&lt;/p&gt;

&lt;p&gt;Now let us consider the following question: Can we name a function from the set
\[
  X \to (Y \to X) ?
\]
What we need to find is a function, which, given an element of \(X\), produces a function from \(Y\) to \(X\). After
a moments thought, it is easy to come up with the function \(x\mapsto C_x\) where \(C_x(y)=x\) for all \(y\in Y\). In
other words, we can send \(x\) to the constant function which only attains the value \(x\).&lt;/p&gt;

&lt;p&gt;In a similar way, one can easily see that
\[
  x\mapsto 1_Y \in X \to (Y \to Y).
\]
However, it seems that the problem has no solution for the sets
\[
  X \to (X \to Y) \;\;\;\text{ and }\;\;\; (X \to Y) \to Y
\]&lt;/p&gt;

&lt;p&gt;We can consider even more complex examples. For instance
\[
  x\mapsto E_x \in X \to ((X \to Y ) \to Y).
\]
where \(E_x\) is the ‘‘evaluation at \(x\)’’ function, that is \(E_x(f) = f(x)\).
Yet the problem has no solution for the set
\[
  X \to ((Y \to X) \to Y).
\]&lt;/p&gt;

&lt;p&gt;As an exercise, you might want to name an element from the set
\[
  (X \to (Y \to Z)) \to ((X \to Y) \to (X \to Z)).
\]&lt;/p&gt;

&lt;p&gt;After seeing all these examples, we have a somewhat vague but very natural questions:
From which sets can we name specific elements? We will give a rather surprising answer to this question.&lt;/p&gt;

&lt;h1 id=&quot;propositional-calculus-with-only-implication&quot;&gt;Propositional Calculus with only Implication&lt;/h1&gt;
&lt;p&gt;After function sets, the title may remind you of the Monty Python Movie &lt;em&gt;And Now for Something Completely Different&lt;/em&gt;
but bear with me. I promise this is going somewhere.&lt;/p&gt;

&lt;p&gt;In this section we will develop a modest theory of propositions. We are not going to define &lt;em&gt;what&lt;/em&gt; a proposition is,
but intuitively, we will think of propositions as judgments. The central notion in our theory will be that of proof,
or rather the “proves” relation which we will denote by \(\vdash\). The proves relation will be a relation
between proposition sets, which we will call assumption sets, and propositions. If \(\Gamma\) is a set of
propositions and \(p\) is a proposition we will write \(\Gamma\vdash p\) to denote \(\Gamma\) proves \(p\).&lt;/p&gt;

&lt;p&gt;Now we will list the properties we want \(\vdash\) to satisfy. These properties will be our deduction rules. For
ease of notation we will abbreviate \(\Gamma\cup\{p\}\) as  \(\Gamma,p\) and \(\Gamma\cup\{p,q\}\) as \(\Gamma,p,q\).
We will also write \(\vdash p\) instead of \(\emptyset\vdash p\).&lt;/p&gt;

&lt;p&gt;Our first deduction rule is trivial: We can prove a proposition if it is among our assumptions. Formally,
\[
  \Gamma, p\vdash p.
\]
We will call this the rule \(A\).&lt;/p&gt;

&lt;p&gt;For the other deduction rules, we need to define how we can construct new propositions from old ones. There are
several ways to do it. We can use conjunctions, disjunctions, negations, implications etc. For the sake of this post, we
will only work with implication. If \(p\) and \(q\) are two propositions then we define a new proposition \(p\to q\)
and read it as “\(p\) implies \(q\)”. Obviously this is a completely formal definition.&lt;/p&gt;

&lt;p&gt;Now we can introduce more deduction rules. The next rule is the famous &lt;em&gt;modus ponens&lt;/em&gt; rule. It says
if one can prove \(p\) and \(p\to q\) from an assumption set \(\Gamma\) then we can also prove \(q\) from \(\Gamma\).
More formally we have
\[
  \text{if }\;\;\;\Gamma\vdash p\;\;\;\text{ and }\;\;\;\Gamma\vdash p\to q
  \;\;\;\text{ then }\;\;\;\Gamma\vdash q.
\]
We will call this the rule \(B\).&lt;/p&gt;

&lt;p&gt;Our final rule is kind of a dual version of modus ponens. It says that if one can prove \(q\) from \(\Gamma\) together
with \(p\) then using only \(\Gamma\) one can prove \(p\to q\). More formally
\[
  \text{if }\;\;\;\Gamma,p\vdash q\;\;\;\text{ then }\;\;\;\Gamma\vdash p\to q.
\]
We will call this the rule \(C\).&lt;/p&gt;

&lt;p&gt;Our propositional logic with the rules \(A\), \(B\) and \(C\) is called the implicational fragment of the intuitionistic propositional
logic and sometimes denoted by \({\rm IP}(\to)\). So what can we deduce in \({\rm IP}(\to)\)? Let us look at a few examples.&lt;/p&gt;

&lt;p&gt;First of all, by \(A\), we have \(p\vdash p\). So, by \(C\), we deduce \(\vdash p\to p\). On the other hand we should not have
\(\vdash p\to q\) because it is absurd to expect an arbitrary proposition to imply an arbitrary proposition. Actually
it is not difficult to prove that we cannot deduce \(\vdash p\to q\) in \({\rm IP}(\to)\) but we will mostly rely on intuitive arguments.&lt;/p&gt;

&lt;p&gt;Here is another example. By \(A\), we have \(p,q\vdash p\). By \(C\), we deduce \(p\vdash q\to p\). Using \(C\) once more we obtain
\(\vdash p\to (q\to p)\). If we start with \(p,q\vdash q\) in the previous argument we obtain \(\vdash p\to (q\to q)\).
On the other hand we should not be able to deduce \(\vdash p \to (p\to q)\). Because assuming \(p\) holds, we cannot
say that \(p\) implies an arbitrary \(y\).&lt;/p&gt;

&lt;p&gt;Note that we have not used the rule \(B\) yet. So let us look at an example involving that rule. Let
\[
  \Gamma=\{p,p\to q\}.
\]
First of all, by \(A\), we have \(\Gamma\vdash p\) and \(\Gamma\vdash p\to q\). Thus, by \(B\), we deduce \(\Gamma\vdash q\).
Now using the rule \(C\) twice we get
\[
  \vdash  p \to ((p \to q) \to q).
\]&lt;/p&gt;

&lt;p&gt;As an exercise, prove that
\[
  \vdash
   (p \to (q \to r)) \to ((p \to q) \to (p \to r)).
\]&lt;/p&gt;

&lt;p&gt;Now let us list the propositions we were able to prove with an empty assumption set:&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp; p \to p, \newline
&amp;amp; p \to (q \to p), \newline
&amp;amp; p \to (q \to q), \newline
&amp;amp; p \to ((p \to q) \to q), \newline
&amp;amp; (p \to (q \to r)) \to ((p \to q) \to (p \to r)).
\end{align}&lt;/p&gt;

&lt;p&gt;Let us also list all the function sets from which we were able to name specific elements:
\begin{align}
&amp;amp; X \to X, \newline
&amp;amp; X \to (Y \to X), \newline
&amp;amp; X \to (Y \to Y), \newline
&amp;amp; X \to ((X \to Y) \to Y), \newline
&amp;amp; (X \to (Y \to Z)) \to ((X \to Y) \to (X \to Z)).
\end{align}&lt;/p&gt;

&lt;p&gt;Either we have a strange coincidence in our hands or we just observed a nontrivial relation between functions and
propositions. In the next post, we will see that this is not a coincidence.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
