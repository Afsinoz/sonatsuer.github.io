<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>The blog of Sonat Süer</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000"/>
 <updated>2018-05-20T00:38:35+03:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Sonat Süer</name>
   <email></email>
 </author>

 
 <entry>
   <title>Kolmogorov Complexity (Part 1 of 2)</title>
   <link href="http://localhost:4000/kolmogorov-complexity/2018/05/20/kolmogorov-complexity-1.html"/>
   <updated>2018-05-20T00:38:35+03:00</updated>
   <id>http://localhost:4000/kolmogorov-complexity/2018/05/20/kolmogorov-complexity-1</id>
   <content type="html">&lt;h1 id=&quot;a-mandatory-example&quot;&gt;A Mandatory Example&lt;/h1&gt;

&lt;p&gt;Consider the following two binary strings:
\begin{align}
&amp;amp;\texttt{0101010101010101010101010101010},\newline
&amp;amp;\texttt{1100100100001111110110101010001}.
\end{align}&lt;/p&gt;

&lt;p&gt;Which one do you think is easier to remember? Most people would say that the first one, because
there is an obvious and simple rule/method to generate it. So, instead of remembering the string,
we can remember the rule. One can also say that the first string is sşmple while the second one is
complex. Kolmogorov complexity is based on this idea.&lt;/p&gt;

&lt;p&gt;Let us semi-formalize it. First let us fix a Turing complete programming language. I will not define what a programming
language &lt;em&gt;is&lt;/em&gt; or &lt;em&gt;should be&lt;/em&gt; and, frankly, be somewhat vague about it. Hence the prefix &lt;em&gt;semi&lt;/em&gt; in front of
&lt;em&gt;formalize&lt;/em&gt;. Though I will make a few assumptions. Our programs in pur programming language will be strings
from from a fixed alphabet \(\mathcal{L}\), say, containing ASCII or UTF-8. I will also assume that each program
accepts strings from \(\mathcal{L}\) as input and produce strings from \(\mathcal{L}\) as output. So,
in particular, we can give a program to another program as input or a program may produce a program. For ease
of reading, I will use \(\texttt{this font}\) when writing a string from \(\mathcal{L}\).&lt;/p&gt;

&lt;p&gt;One technical assumtion which will be important later is that our program will use decimal representation
for natural numbers. Actually pretty much any positional system would do, but for the sake of definiteness
I will stick with decimal.&lt;/p&gt;

&lt;p&gt;When I write a program, I will use some sort of pseudocode hoping that there is no ambiguity. Also I will
not exclude partial programs which corresponds do partial functions. For insance
\begin{align}
&amp;amp;\texttt{accept x as input}\newline
&amp;amp;\texttt{while 1 == 1}\newline
&amp;amp;\;\;\;\texttt{do nothing}\newline
&amp;amp;\texttt{return x}
\end{align}
is a valid program even though it does not produce any result as it is always stuck in the while loop.&lt;/p&gt;

&lt;p&gt;Now we can define the Kolmogorov complexity of a string from \(\mathcal{L}\): Let \(\sigma\) be a string
from \(\mathcal{L}\). The Kolmgorov complexity of \(\sigma\) is the length of the shortest program which
generates \(\sigma\) on any input. It is denoted by \(K(\sigma)\).&lt;/p&gt;

&lt;h1 id=&quot;how-canonical-is-this-definition&quot;&gt;How canonical is this definition?&lt;/h1&gt;

&lt;p&gt;The definition seems very intuitive, especially after the mandatory example. However, there is something fishy here.
We made several arbitrary choices yet we called \(K(\sigma)\) &lt;em&gt;the&lt;/em&gt; Kolmogorov complexity of \(\sigma\). It should
be clear that it is easier to generate certain strings in some langugaes than others. Indeed, the second string in our
example is just the first few digits of the binary expansion of \(\pi\) so it can be easier to generate in a language
designed for numerical computations.&lt;/p&gt;

&lt;p&gt;Actually, for any string \(\sigma\), we can very well imagine a programming language which has \(\sigma\) as a built-in
constant. Thus the Kolmogorov complexity of a single string seems to very heavily depend on our language of choice. So what
happens if we change our programming language? Does this mean that our notion is arbitrary?&lt;/p&gt;

&lt;p&gt;Fortunately, no. Let us consider two notions of complexity \(K_1\), \(K_2\) corresponding
to two different programming languages \(L_1\) and \(L_2\), respectively. Since \(L_1\) is Turing complete, we can write a
&lt;em&gt;compiler&lt;/em&gt; for \(L_2\) in \(L_1\). In other words, \(L_1\) can simulate \(L_2\). Let \(\sigma\) be a string and
let \(n=K_2(\sigma)\). Then there is a program \(p\) in the language \(L_2\) which witnesses the fact that \(n=K_2(\sigma)\).
That is, \(p\) has length \(n\) and \(p\) generates \(\sigma\). Now consider the (description of a) program in \(L_1\):
Simulate \(p\). The length of this program will be \(n + c\) for some \(c\) becuse the program contains the string \(p\) and
\(p\) has length \(n\). The part corresponding to \(c\) is the part that simulates \(L_2\) and it does not depend on \(\sigma\).
By construction, “Simulate \(p\)” generates \(\sigma\), thus
\[
  K_1(\sigma)\leq n + c = K_2(\sigma) + c.
\]
We proved that there is a constant \(c\) such that \(K_1(\sigma)\leq K_2(\sigma) + c\) for all \(\sigma\). By stmmetry,
there is a \(c’\) such that \(K_2(\sigma)\leq K_1(\sigma) + c’\) for all \(\sigma\). Combining these two we obtain
\[
  |K_1(\sigma) - K_2(\sigma)| &amp;lt; {\rm max}\{c, c’\}.
\]
This shows that if we consider an infinite family of strings and consider the asymptotic behaviour of Kolmogorov complexity
on that family, then the programming language we choose does not matter. Obviously, this does not imply that this asymptotic
behaviour is easy to determine.&lt;/p&gt;

&lt;p&gt;From now on we will stick with a fixed but not explicitely determined choice of language and denote the complexity function
given by that language \(K\).&lt;/p&gt;

&lt;h1 id=&quot;berry-paradox&quot;&gt;Berry Paradox&lt;/h1&gt;

&lt;h1 id=&quot;an-incomputability-result&quot;&gt;An Incomputability Result&lt;/h1&gt;

</content>
 </entry>
 
 <entry>
   <title>Curry-Howard Corespondence From Scratch (Part 2 of 2)</title>
   <link href="http://localhost:4000/curry-howard/2018/05/14/curry-howard-from-scratch-2.html"/>
   <updated>2018-05-14T00:00:00+03:00</updated>
   <id>http://localhost:4000/curry-howard/2018/05/14/curry-howard-from-scratch-2</id>
   <content type="html">&lt;h1 id=&quot;simply-typed-lambda-calculus&quot;&gt;Simply Typed Lambda Calculus&lt;/h1&gt;

&lt;p&gt;In the first post we constructed a formal theory of propositions but left the
notion of &lt;em&gt;naming a specific element&lt;/em&gt; from a function set vague. Now we are going to fix that.&lt;/p&gt;

&lt;p&gt;Our central notion will be that of a &lt;em&gt;lambda term&lt;/em&gt;. As we want to define a formal theory we will
not define what a lambda term is and focus on how to construct new lamda terms from old ones. Intuitively
though, we will think of lamnda terms as function prototypes.&lt;/p&gt;

&lt;p&gt;First we need variables. We will assume that we have an infinite set of variables and denote
variables by lower case letters from the Latin alphabet. Each variable will also be a lambda term.
Other than that, therer will be two ways to construct a lambda term. First, if \(M\) and \(N\) are
lambda terms then so is \(MN\). We will think of \(MN\) as \(M\) applied to \(N\). Second,
if \(x\) is a vaiable and \(M\) is a lamda term than so is \(\lambda x.M\). Intuiitively
\(\lambda x.M\) will mean \(x\mapsto M(x)\). This will be clearer when we start using lambda terms.&lt;/p&gt;

&lt;p&gt;Let us look at a few examples. The follwoing are all lambda terms constructed using varibales only:
\[
  x,\; xy,\; xx,\; x(yz),\; (xy)z,\; (xx)x,\; (xy)(x(yz)).
\]
Of course we can also use \(\lambda\):
\[
  \lambda x.x,\; \lambda y. y,\; \lambda x. y,\; \lambda y. (\lambda x. (yx))),\;
  (\lambda x. (xy))(\lambda y. x).
\]
To reduce the number of bracktes we will give application higher prority than \(\lambda\).
So, for instance, the expression \(\lambda x. yz\) will stand for \(\lambda x. (yz)\) rather than
\((\lambda x. y)z\).&lt;/p&gt;

&lt;p&gt;Now let us define when two lambda terms are equal. Since we think of \(\lambda x.M\) as a function
mapping \(x\) to \(M\), we see \(\lambda x.x\) as some kind of identity function. However, under
this interpretation, there should not be a difference between \(\lambda x.x\) and \(\lambda y.y\).
More generally there should not be a difference between \(x\mapsto M(x)\) and \(y\mapsto M(y)\).
Therefore we want
\[
  \lambda x. x  = \lambda y. y = \lambda z. z = \ldots
\]
Maybe slightly more interestingly, we also want
\begin{align}
\lambda z. ((\lambda x.x)(\lambda y.yz))   &amp;amp;=
\lambda u. ((\lambda x.x)(\lambda y.yu))\newline &amp;amp;=
\lambda z. ((\lambda x.x)(\lambda u.uz))\newline &amp;amp;=
\lambda z. ((\lambda z.z)(\lambda u.uz))\newline &amp;amp;=
\ldots
\end{align}&lt;/p&gt;

&lt;p&gt;We will call this &lt;em&gt;harmless renaming of variables&lt;/em&gt; \(\alpha\)-equivalence. One can define \(\alpha\)-equivalence
formally but it is slightly messy. Therefore we will not do it here and instead adopt a convention: In a \(\lambda\)
term, each \(\lambda\) will use a different variable. So, for instance, we will avoid using the last lambda term
in the list above since \(\lambda z\) appears twice in it.&lt;/p&gt;

&lt;p&gt;Now comes the more interesting properties of eqaulity. We define
\[
  (\lambda x. M)N = M[x:= N].
\]
Here \(M\) is a lambda term and \( M[x:= N]\) stands for the lambda term obtained from \(M\) by replacing each
occurance of \(x\) by \(N\). For instance
\[
  (\lambda x. x) N = x[x := N] = N.
\]
So \(\lambda x. x\) does behave like identity. Here is another example:
\begin{align}
(\lambda y. (\lambda x. yx))(\lambda z. z)  &amp;amp;= (\lambda x. yx)[y:=\lambda z. z] \newline
&amp;amp;= \lambda x. ((\lambda z.z)x) \newline
&amp;amp;= \lambda x. (z[z:=x]) \newline
&amp;amp;= \lambda x. x.
\end{align}&lt;/p&gt;

&lt;p&gt;Finally, let us express the functions \(C\) and \(E\) we defined in the previos post as lambda terms. By definition
\(C_x(y)=x\) so \(C_x = \lambda y. x\). If we view \(C_x\)  as a function of \(x\) then we get
\[
  C = \lambda x. (\lambda y. x).
\]
Again by definition we have \(E_x(f) = f(x)\) so \(E_x = \lambda f. f x\). Therefore
\[
  E = \lambda x. (\lambda f. f x).
\]&lt;/p&gt;

&lt;p&gt;What we defined so far is called the &lt;em&gt;untyped lambda calculus&lt;/em&gt;. Even though it gives us a formal theory of
functions, it is not enough for our purposses as we also need function sets. To be able to talk about some
sort of function sets, we will introduce the notion of &lt;em&gt;type&lt;/em&gt; into our theory.&lt;/p&gt;

&lt;p&gt;The definition of a type is going to be very simple: if \(\alpha\) and  \(\beta\) are types then so is
\(\alpha\to\beta\). Note that, formally speaking, there is no difference between types and propositions we defined in the
previous post. On the other hand, while a propositon has the connotation of a judgement, a type will be
more like function sets.&lt;/p&gt;

&lt;p&gt;Now we will define our typing relation between lambda terms and types. If a lambda term \(M\) is related to the type
\(\tau\) then we will say that \(M\) is of type \(\tau\) and denote it by \(M\colon\tau\). We will call
a statemnt like \(M\colon\tau\) a type assignment. A context will simply be a set of type assignments. Our typing system
will consist of rules, which given a context, allows us to derive type assignments. If a context \(\Gamma\) allows us
to derive an assignment \(M\colon\tau\) we will denote it by \(\Gamma\vdash M\colon\tau\). Note that we are overloading
the symbol \(\vdash\) here.&lt;/p&gt;

&lt;p&gt;Here is the first rule. Let \(\Gamma\) be a context, then
\[
  \Gamma,x\colon\tau\vdash x\colon\tau\
\]
So, if the type of varible \(x\) is \(\tau\) in a given context, then we can derive the assignment \(x\colon\tau\)
in that context. Let us call this the rule \(A^\to\).&lt;/p&gt;

&lt;p&gt;Here is the second rule:
\[
  \text{if }\;\,
  \Gamma\vdash N:\sigma\;\,
  \text{ and }\;\,\
  \Gamma\vdash M:\sigma\to\tau\;\;
  \text{ then }\;\;\Gamma\vdash MN:\tau.
\]
So if \(M\) is a function which maps objects of type \(\alpha\) to objects of type \(\beta\) and
\(N\) is of type \(\alpha\) then \(M\) applied to \(N\) should be of type \(\beta\). Let us call this
the rule \(B^\to\).&lt;/p&gt;

&lt;p&gt;Finally,
\[
  \text{if }\;\;\;
  \Gamma,x:\tau\vdash M:\sigma \;\;\;
  \text{ then }\;\;\;
  \Gamma\vdash \lambda x. M:\tau\to\sigma.
\]
If \(x\) is of type \(\tau\) then any lambda term starting with \(\lambda x.\) should be a function
from \(\tau\) to somewhere. As \(M\) is of type \(\sigma\), obviously we should have \(\lambda x. M:\tau\to\sigma\).
Let us call this the rule \(C^\to\).&lt;/p&gt;

&lt;p&gt;The system we constructed with rules \(A^\to\), \(B^\to\) and \(C^\to\) is called the simply typed
lambda calculus in the style of Church and sometimes denoted by \(\lambda^\to\). At this point you might
want to compare \({\rm IP}(\to)\) and \(\lambda^\to\), especially the rules \(A\), \(B\), \(C\)
and \(A^\to\), \(B^\to\), \(C^\to\).&lt;/p&gt;

&lt;p&gt;Now we can give a precise definition of naming a specific element from a function set. Let \(A\) be a function
set as we used in the previous post. Formally, we can view \(A\) as a type, say \(\tau\). If there is a
lambda term \(M\) such that \(\vdash M\colon\tau\) then we say that one can name a specific function from
\(A\), namely \(M\).&lt;/p&gt;

&lt;p&gt;As an example consider the lambda term \(\lambda x.(\lambda f.f x)\). Recall that it corresponds to
\(x\mapsto E_x\). Let \(\Gamma=\{x:\alpha,\,f:\alpha\to\beta\}\). Then, by the rule \(A^\to\), we
get
\[
  \Gamma\vdash x\colon\alpha \;\;
  \text{ and }\;\;
  \Gamma\vdash f\colon\alpha\to\beta.
\]
And from these, together with rule \(B^\to\) we obtain \(\Gamma\vdash f x: \beta\). Finally, applying
the rule \(C^\to\) twice we derive
\[
  \vdash \lambda x. (\lambda f. f x) : \alpha\to((\alpha\to\beta)\to\beta).
\]&lt;/p&gt;

&lt;h1 id=&quot;finally-the-curry-howard-correspondence&quot;&gt;Finally, The Curry-Howard Correspondence&lt;/h1&gt;
&lt;p&gt;We need one final definition before we can express the Curry-Howard correspondence between \({\rm IP}(\to)\) and
\(\lambda^\to\) formally.&lt;/p&gt;

&lt;p&gt;Let \(\Gamma\) be a context. Define
\[
  |\Gamma| =\{\tau \colon \text{ there is a variable $x$ such that } x : \tau\in\Gamma\}.
\]
Note that \(|\Gamma|\) consists of types/propositions. In other words, \(|\Gamma|\) is an assumption set. is the set of variables
appearing in \(\Gamma\).&lt;/p&gt;

&lt;p&gt;Finally, I present you the Curry-Howard Correspondence for \({\rm IP}(\to)\) and \(\lambda^\to\):&lt;/p&gt;

&lt;p&gt;If \(\Gamma\vdash M:\varphi\) then \(|\Gamma|\vdash\varphi\). Conversely, if \(|\Gamma|\vdash\varphi\) then
there is a lambda term \(M\) such that \(\Delta\vdash M:\varphi\) where \(\Delta=\{x_\psi:\psi\;|\; \psi\in\Gamma\}\).
To paraphrase it as a slogan, if we view types as propositions, provoable propositions are precisley the types of
lambda terms.&lt;/p&gt;

&lt;p&gt;Even this modest version of the Curry-Howard corrrespondence is beautiful and surprising. One wonders if it is possible to
generalize it. Actually, there is a very natural direction of generalization. We know that \({\rm IP}(\to)\) is a restricted version
of a more general logic, namely the full intuitionistic propositional logic. So we can try to generalize the theorem
to different logics. However, it is not clear how to generalize \(\lambda^\to\), especially if you have not seen lambda calculus
before. The idea is that \(\lambda^\to\) is a programming language and in order to generalize the Curry-Howard correspondence, we need
to find/invent different programming languages with different type systems.&lt;/p&gt;

&lt;p&gt;First, let us clarify what we mean by \(\lambda^\to\) is a programming language. Let us fix a type \(\tau\) and define
\(\texttt{Nat}=(\tau\to\tau)\to(\tau\to\tau)\). As the name suggests, \(\texttt{Nat}\) will be the set of natural
numbers. For lambda terms \(M\) and \(N\) let us make the following recursive definition: \(M^0N = N\) and
\(M^{n+1}N=M(M^nN)\). So \(M^nN\) is \(M\) applied to \(N\), \(n\)-times. The Church encoding
of a natural number \(n\) is
\[
  \ulcorner n \urcorner = \lambda s. (\lambda z. s^n z).
\]
As you can check easily, \(\vdash \ulcorner n \urcorner : \texttt{Nat}\). We can also define basic operations on \(\texttt{Nat}\).
For instnce if
\[
  A_+ = \lambda x. (\lambda y. (\lambda s. (\lambda z. (xs)((ys)z))))
\]
then we have \(\vdash A_+ : \texttt{Nat} \to (\texttt{Nat} \to \texttt{Nat})\) and for all natural number \(m\) and \(n\) the
equality \((A_+\ulcorner m \urcorner )\ulcorner n \urcorner =\ulcorner m + n \urcorner \) holds. Therefore we can interpret
\(A_+\) as a program which adds numbers. As an exercise you might want to define a computer program which does multiplication.&lt;/p&gt;

&lt;p&gt;One can even define booleans in \(\lambda^\to\). Let \({\bf T}=\lambda y. (\lambda x . x)\) and \({\bf F}=\lambda y. (\lambda x . y)\).
If we interpret \({\bf T}\) as true and \({\bf F}\) as false then \((BP)Q\) acts like
\[
  {\bf if}\; B\; {\bf then}\; P \;{\bf else}\; Q
\]
if \(B\) is \({\bf T}\) of \({\bf F}\). These may look like ad-hoc ideas but lambda terms (without their types) is actually a
universal way of writing programs. This is the topic of another post, though.&lt;/p&gt;

&lt;p&gt;Now back to the original question (with a finer statement): Can we find/invent a different programming language whose typing rules
corresponds to a logic in such a way that proving a proposition/type in that logic corresponds to writing a program of that
proposition/type? The answer is amazingly yes!&lt;/p&gt;

&lt;p&gt;For a very simple example, consider the &lt;em&gt;and&lt;/em&gt; operation on propositions.
The type construction rule corresponding to logical &lt;em&gt;and&lt;/em&gt; should be pairing, also known as the Cartesian product. Because, using the analogy
in the first post, naming an element from a set \(X\) &lt;em&gt;and&lt;/em&gt;  a set \(Y\) is the same as naming an element from \(X\times Y\).
In a similar fashion, &lt;em&gt;or&lt;/em&gt; corresponds to sum types. One can also consider logics with quantifiers, modalities or linear logics etc. and they
all come with their corresponding programming languages. Moreover the properties of the type system viewed as a logic are reflected as
programming language features such as polymorphism, staging, resource awareness etc. So what we covered in these two posts is just the tip
of the iceberg.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Curry-Howard Corespondence From Scratch (Part 1 of 2)</title>
   <link href="http://localhost:4000/curry-howard/2018/05/05/curry-howard-from-scratch-1.html"/>
   <updated>2018-05-05T00:00:00+03:00</updated>
   <id>http://localhost:4000/curry-howard/2018/05/05/curry-howard-from-scratch-1</id>
   <content type="html">&lt;h1 id=&quot;function-sets&quot;&gt;Function Sets&lt;/h1&gt;

&lt;p&gt;Suppose we have two sets \(X\) and \(Y\). Can we name a function from \(X\) to \(Y\) without
using what \(X\) or \(Y\) is? Obviously if \(Y\) is empty and \(X\) is not empty then this is not possible
as there is no function from a nonempty set to the empty set. So let us suppose \(X\) and \(Y\) are nonempty
for the moment. Now there &lt;em&gt;are&lt;/em&gt; functions from \(X\) to \(Y\). For instance, for every element \(y\in Y\),
there is the constant function which takes the value \(y\). Bu this does not count because the problem is to &lt;em&gt;name&lt;/em&gt; a
particular function, not to prove the existence of such functions. It looks difficult. Actually, in a sense
which we will define later, the answer is that we cannot name such a function. Considering the fact that we do not
know anything about \(X\)  and \(Y\) this is hardly surprising.&lt;/p&gt;

&lt;p&gt;On the other hand, if we take \(X=Y\) then it is possible to name such a function: the identity function. Since
the identity function maps each element to itself, we do not need to know anything about \(X\). Frankly, this
does not seem like an interesting observation, either. However, as we consider more complex examples, the situation will
change dramatically.&lt;/p&gt;

&lt;p&gt;Let us introduce some notation. For sets \(X\) and \(Y\), we will denote the set of functions from \(X\) to \(Y\)
by \(X\to Y\). In this notation \(f \in X\to Y\) and \(f \colon X \to Y\) have the same meaning. For instance,
if we denote the identity function on \(X\) by \(1_X\), then we have \(1_X \in X\to X\).&lt;/p&gt;

&lt;p&gt;Now let us consider the following question: Can we name a function from the set
\[
  X \to (Y \to X) ?
\]
What we need to find is a function, which, given an element of \(X\), produces a function from \(Y\) to \(X\). After
a moments thought, it is easy to come up with the function \(x\mapsto C_x\) where \(C_x(y)=x\) for all \(y\in Y\). In
other words, we can send \(x\) to the constant function which only attains the value \(x\).&lt;/p&gt;

&lt;p&gt;In a similar way, one can easily see that
\[
  x\mapsto 1_Y \in X \to (Y \to Y).
\]
However, it seems that the problem has no solution for the sets
\[
  X \to (X \to Y) \;\;\;\text{ and }\;\;\; (X \to Y) \to Y
\]&lt;/p&gt;

&lt;p&gt;We can consider even more complex examples. For instance
\[
  x\mapsto E_x \in X \to ((X \to Y ) \to Y).
\]
where \(E_x\) is the ‘‘evaluation at \(x\)’’ function, that is \(E_x(f) = f(x)\).
Yet the problem has no solution for the set
\[
  X \to ((Y \to X) \to Y).
\]&lt;/p&gt;

&lt;p&gt;As an exercise, you might want to name an element from the set
\[
  (X \to (Y \to Z)) \to ((X \to Y) \to (X \to Z)).
\]&lt;/p&gt;

&lt;p&gt;After seeing all these examples, we have a somewhat vague but very natural questions:
From which sets can we name specific elements? We will give a rather surprising answer to this question.&lt;/p&gt;

&lt;h1 id=&quot;propositional-calculus-with-only-implication&quot;&gt;Propositional Calculus with only Implication&lt;/h1&gt;
&lt;p&gt;After function sets, the title may remind you of the Monty Python Movie &lt;em&gt;And Now for Something Completely Different&lt;/em&gt;
but bear with me. I promise this is going somewhere.&lt;/p&gt;

&lt;p&gt;In this section we will develop a modest theory of propositions. We are not going to define &lt;em&gt;what&lt;/em&gt; a proposition is,
but intuitively, we will think of propositions as judgements. The central notion in our theory will be that of proof,
or rather the “proves” relation which we will denote by \(\vdash\). The proves relation will be a relation
between proposition sets, which we will call assumption sets, and propositions. If \(\Gamma\) is a set of
propositions and \(p\) is a propositon we will write \(\Gamma\vdash p\) to denote \(\Gamma\) proves \(p\).&lt;/p&gt;

&lt;p&gt;Now we will list the properties we want \(\vdash\) to satisy. These properties will be our deduction rules. For
ease of notation we will abbreviate \(\Gamma\cup\{p\}\) as  \(\Gamma,p\) and \(\Gamma\cup\{p,q\}\) as \(\Gamma,p,q\).
We will also write \(\vdash p\) instead of \(\emptyset\vdash p\).&lt;/p&gt;

&lt;p&gt;Our first deduction rule is trivial: We can prove a proposition if it is among our assumptions. Formally,
\[
  \Gamma, p\vdash p.
\]
We will call this the rule \(A\).&lt;/p&gt;

&lt;p&gt;For the other deduction rules, we need to define how we can construct new propositions from old ones. There are
several ways to do it. We can use conjunctions, disjunctions, negations, implications etc. For the sake of this post, we
will only work with implication. If \(p\) and \(q\) are two propositions then we define a new propositon \(p\to q\)
and read it as “\(p\) implies \(q\)”. Obviously this is a completely formal definition.&lt;/p&gt;

&lt;p&gt;Now we can introduce more deduction rules. The next rule is the famous &lt;em&gt;modus ponens&lt;/em&gt; rule. It says
if one can prove \(p\) and \(p\to q\) from an assumption set \(\Gamma\) then we can also prove \(q\) from \(\Gamma\).
More formally we have
\[
  \text{if }\;\;\;\Gamma\vdash p\;\;\;\text{ and }\;\;\;\Gamma\vdash p\to q
  \;\;\;\text{ then }\;\;\;\Gamma\vdash q.
\]
We will call this the rule \(B\).&lt;/p&gt;

&lt;p&gt;Our final rule is kind of a dual version of modus ponens. It says that if one can prove \(q\) from \(\Gamma\) together
with \(p\) then using only \(\Gamma\) one can prove \(p\to q\). More formally
\[
  \text{if }\;\;\;\Gamma,p\vdash q\;\;\;\text{ then }\;\;\;\Gamma\vdash p\to q.
\]
We will call this the rule \(C\).&lt;/p&gt;

&lt;p&gt;Our propositional logic with the rules \(A\), \(B\) and \(C\) is called the implicational fragment of the intutionistic propositional
logic and sometimes denoted by \({\rm IP}(\to)\). So what can we deduce in \({\rm IP}(\to)\)? Let us look at a few examples.&lt;/p&gt;

&lt;p&gt;First of all, by \(A\), we have \(p\vdash p\). So, by \(C\), we deduce \(\vdash p\to p\). On the other hand we should not have
\(\vdash p\to q\) because it is absurd to expect an arbitrary propositon to imply an arbitrary proposition. Actually
it is not difficult to prove that we cannot deduce \(\vdash p\to q\) in \({\rm IP}(\to)\) but we will mostly rely on intuitive arguments.&lt;/p&gt;

&lt;p&gt;Here is another example. By \(A\), we have \(p,q\vdash p\). By \(C\), we deduce \(p\vdash q\to p\). Using \(C\) once more we obtain
\(\vdash p\to (q\to p)\). If we start with \(p,q\vdash q\) in the previous argument we obtain \(\vdash p\to (q\to q)\).
On the other hand we should not be able to deduce \(\vdash p \to (p\to q)\). Becasue assuming \(p\) holds, we cannot
say that \(p\) implies an arbitrary \(y\).&lt;/p&gt;

&lt;p&gt;Note that we have not used the rule \(B\) yet. So let us look at an example involving that rule. Let
\[
  \Gamma=\{p,p\to q\}.
\]
First of all, by \(A\), we have \(\Gamma\vdash p\) and \(\Gamma\vdash p\to q\). Thus, by \(B\), we deduce \(\Gamma\vdash q\).
Now using the rule \(C\) twice we get
\[
  \vdash  p \to ((p \to q) \to q).
\]&lt;/p&gt;

&lt;p&gt;As an exercise, prove that
\[
  \vdash
   (p \to (q \to r)) \to ((p \to q) \to (p \to r)).
\]&lt;/p&gt;

&lt;p&gt;Now let us list the propositions we were able to prove with an empty assumtion set:&lt;/p&gt;

&lt;p&gt;\begin{align}
&amp;amp; p \to p, \newline
&amp;amp; p \to (q \to p), \newline
&amp;amp; p \to (q \to q), \newline
&amp;amp; p \to ((p \to q) \to q), \newline
&amp;amp; (p \to (q \to r)) \to ((p \to q) \to (p \to r)).
\end{align}&lt;/p&gt;

&lt;p&gt;Let us also list all the function sets from which we were able to name specific elements:
\begin{align}
&amp;amp; X \to X, \newline
&amp;amp; X \to (Y \to X), \newline
&amp;amp; X \to (Y \to Y), \newline
&amp;amp; X \to ((X \to Y) \to Y), \newline
&amp;amp; (X \to (Y \to Z)) \to ((X \to Y) \to (X \to Z)).
\end{align}&lt;/p&gt;

&lt;p&gt;Either we have a strange coincidence in our hands or we just observed a nontrivial relation between functions and
propositions. In the next post, we will see that this is not a coincidence.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
